#### 消息队列

##### 消息队列作用/好处

* 异步
  * 同步：类似于食堂打饭，你需要等待食堂大妈完成所有步骤(加个鸡腿、加点汤、加点饭)，整个过程需要**同步等待**
  * 异步：类似于饭店点菜，通知服务员点的菜(传达一个消息)，之后，等上菜期间你就能跟自己的事

* 解耦
  * 比如购票流程：购票系统->发送短信->发送邮件，在此购票方法中，若想添加新的功能，如添加积分，或者删去发送邮件的模块，则需要频繁改代码，重启应用
  * 此时我们可以使用消息队列对过程进行解耦，因为之后的操作都依赖于第一步购票系统的result(如订单号、用户身份信息等)，因此可以使用类似"广播消息"来实现，也就是消息队列中的**订阅Topic**。
  * **生产者**(购票系统)将**消息**(购票result)放到特定的主题中，**消费者**(短信系统、邮件系统、积分系统)**只需关注从指定的Topic中拉取消息**即可。每个系统各司其职，实现了解耦

* 削峰
  * 购票系统中，若并发量很高，短信系统压力会很高，我们使用消息队列做缓冲，让短信系统**尽自己最大能力去消息队列取消息和消费消息**，而不至于系统直接崩溃。就如收验证码不是一发就收到，有延时但能接受

##### 消息队列的副作用

* 重复消费消息？
* 消息的顺序消费？
* 分布式事务问题？生产者生产很快，消费者消费很慢？消息堆积？



#### RocketMQ

高性能、高可靠、高实时、分布式

#### 队列模型和主题模型

* 队列模型就是一队列。。单个队列很难满足需求

* 主题模型/发布订阅模型：消息生产者是发布者，消息消费者是订阅者，存放消息的容器成为主题

#### RocketMQ的消息模型

采用主题模型

![image-20210320134412888](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320134412888.png)

* Producer Group生产者组：代表一类生产者，如n个秒杀系统，他们一般生产相同的消息
* Consumer Group消费者组：代表一类消费者，如短信系统，他们一般消费相同的消息
* Topic主题：代表一类消息，如订单消息、库存消息等

生产者向主题发送消息，主题中有**多个消息队列**，一个消费者组多台机器共同消费一个Topic中的多个队列，**一个队列只会被一个消费者消费**。一个消费者挂了，组内的其他消费者会顶替并继续消费。一般来讲控制消费者组的消费者数与主题的队列数相同。

**每个消费组在每个队列上维护一个消费位置**。因为消费者组不止一个，每个组需要用一个**消费位移(位置)**标记本组消费到哪儿了，每次消费完会返回一个成功的响应，此时消费位移加1，这样就不会重复消费了。不删除消息，因为别的组还要用~

![image-20210320135729926](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320135729926.png)

**每个Topic为啥要多个队列嘞？**为了提高**并发能力**

总结：一个Topic中配置多个队列并且每个队列维护各个消费者组的消费位置

#### RocketMQ的架构图

四大角色：NameServer、Broker、Producer、Consumer

* Broker：负责消息存储、投递和查询以及服务高可用保证。就是消息队列服务器，生产者往里丢消息，消费者从里面拉取消息

  > 一个Topic分布在多个Broker上，一个Broker可以配置多个Topic，多对多
  >
  > 若某个Topic消息量很大，可以将其分布在不同的Broker上，减轻单个Broker的压力，从而提高并发
  >
  > 一般来说，消息量均匀，某个Broker队列越多，压力越大

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320140753300.png" alt="image-20210320140753300" style="zoom:67%;" />

* NameServer：注册中心。**Broker管理**和**路由信息管理**。Broker将自己的信息注册到NameServer中，并定期向NameServer发送心跳包(自身的主题信息)。生产者和消费者也会定期向NameServer查询相关信息。生产者和消费者从NameServer中获取路由表，按照路由表的信息和对应的Broker进行通信

* Producer：生产者，可分布式部署

* Consumer：消费者，可分布式部署。

  ![image-20210320141215605](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320141215605.png)

  NameServer管理Broker，以保证负载均衡，对个Broker也实现了解耦

  ![image-20210320141359621](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320141359621.png)

  * 细节补充：
    * Broker可以做集群，还可以进行主从部署(master/slave)。slave定时从master同步数据(同步刷盘或者异步刷盘)，若master宕机，**则slave提供消费服务，但是不能写入消息**
    * 为保证高可用，NameServer也做了集群部署，它是**去中心化**的，即没有主节点。RocketMQ中通过**单个Broker和所有NameServer保持长连接**，并每30秒Broker向所以NameServer发送心跳(自身Topic信息)
    * 生产者向Broker发送消息前，需要先从NameServer获取路由信息，然后通过**轮询**的方式向每个队列中产生数据，已达到负载均衡
    * 消费者通过NameServer后去Broker路由信息，向Broker发送Poll请求来获取消息数据。有**广播**和**集群**两种方式

#### 如何解决顺序消费和重复消费？

##### 顺序消费

普通顺序：同一个消费队列收到的消息是有序的，不同队列可能无序。普通消息在Broker重启情况下不保证有序

严格顺序：所有消息均有序。代价大

因此一般采用普通顺序。

如秒杀系统中，创建订单、支付、发货三个消息，按照轮询策略可能会被发送到不同的队列，如何保证有序呢？只需将同一语义下的消息放入同一队列中(如同一个订单)，可以采用**Hash取模法**来保证

##### 重复消费

**幂等**，即任意多次执行所产生的影响均与一次执行的影响相同。

可以写入Redis来保证，Redis的key和value天然支持幂等。

可以用数据库插入法，基于数据库的唯一键来保证重复数据不会被插入多条

#### 分布式事务

事务：要么都执行，要么都不执行

在分布式系统中如何保证呢？

RocketMQ使用**事务消息加上事务反查机制**

![image-20210320143428242](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320143428242.png)

在第一步发送的 half 消息 ，它的意思是 **在事务提交之前，对于消费者来说，这个消息是不可见的** ，即不能被消费。

> 那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将**备份**原消息的主题与消息消费队列，然后 **改变主题** 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端**无法消费**half类型的消息，**然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费**，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。

你可以试想一下，如果没有从第5步开始的 **事务反查机制** ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 `RocketMQ` 中就是使用的上述的事务反查来解决的，而在 `Kafka` 中通常是直接抛出一个异常让用户来自行解决。

你还需要注意的是，在 `MQ Server` 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——**本地事务和存储消息到消息队列才是同一个事务**。这样也就产生了事务的**最终一致性**，因为整个过程是异步的，**每个系统只要保证它自己那一部分的事务就行了**。

#### 消息堆积问题

将消息放入消息队列可以起到削峰的作用。但如果峰值过大，导致消息堆积在队列中，咋办？

增加消费者实例，**同时需要增加每个主题的队列数量**，因为一个队列只会被一个消费者消费

检查是否是消费者出现了大量消费错误

#### 回溯消费

回溯消费是指 `Consumer` 已经消费成功的消息，由于业务上需求需要重新消费，在`RocketMQ` 中， `Broker` 在向`Consumer` 投递成功消息后，**消息仍然需要保留** 。并且重新消费一般是按照时间维度，例如由于 `Consumer` 系统故障，恢复后需要重新消费1小时前的数据，那么 `Broker` 要提供一种机制，可以按照时间维度来回退消费进度。`RocketMQ` 支持按照时间回溯消费，时间维度精确到毫秒。

#### RocketMQ 的刷盘机制

##### 同步刷盘和异步刷盘

保证数据的持久化，内存与磁盘都存一份数据

![image-20210320144847949](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320144847949.png)

需要等待一个刷盘成功的**ACK**，可靠性较好，**性能影响较大**

异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， **降低了读写延迟** ，提高了 `MQ` 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。

一般地，**异步刷盘只有在 `Broker` 意外宕机的时候会丢失部分数据**

#### 同步复制和异步复制

上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 `Borker` **主从模式**下，主节点返回消息给客户端的时候是否需要同步从节点。

* 同步复制：也叫“同步双写”，消息都写入到主从节点后才返回写入成功
* 异步复制：消息写入主节点后就返回写入成功

需要根据实际情况，看是更需要可靠性，还是更需要性能

**异步复制会不会也像异步刷盘那样影响消息的可靠性呢？**

不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 **可用性** 。为什么呢？其主要原因**是 `RocketMQ` 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了**。

比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，**消费者可以自动切换到从节点进行消费**(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。

在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？**一个主从不行那就多个主从的呗**，别忘了在我们最初的架构图中，每个 `Topic` 是分布在不同 `Broker` 中的。

这回导致无法保证严格顺序。

#### 存储机制

`Topic` 中的 **队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？**

RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile

* CommitLog：**消息主体以及元数据的存储主体**，存储 `Producer` 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认**1G** ，文件名长度为**20位**，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是**顺序写入日志文件**，当文件满了，写入下一个文件。

* ConsumeQueue： 消息消费队列，**引入的目的主要是提高消息消费的性能**，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）**作为消费消息的索引**，保存了指定 Topic 下的队列消息在 CommitLog 中的**起始物理偏移量** offset ，**消息大小 size 和消息 Tag 的 HashCode 值**。consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M；

- `IndexFile`： `IndexFile`（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。

![image-20210320153400989](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320153400989.png)

`RocketMQ` 采用的是 **混合型的存储结构** ，即为 `Broker` 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 `Kafka` 中会为每个 `Topic` 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，`RockeMQ` 是不分书的种类直接成批的塞上去的，而 `Kafka` 是将书本放入指定的分类区域的。

而 `RocketMQ` 为什么要这么做呢？原因是 **提高数据的写入效率** ，不分 `Topic` 意味着我们有更大的几率获取 **成批** 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。

所以，在 `RocketMQ` 中又**使用了 `ConsumeQueue` 作为每个队列的索引文件**来 **提升读取消息的效率**。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。

#### RocketMQ存储架构图

![image-20210320153600076](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210320153600076.png)

为什么 `CommitLog` 文件要设计成固定大小的长度呢？提醒：**内存映射机制**。







#### 相关问题与解答：

https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/message-queue/RocketMQ-Questions






- - -
参考：

https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/message-queue/RocketMQ

https://snailclimb.gitee.io/javaguide/#/docs/system-design/distributed-system/message-queue/RocketMQ-Questions