

#### 电商秒杀项目基础架构

#### 1. 项目架构

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316093317847.png" alt="image-20210316093317847" style="zoom: 67%;" />

------



<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316134312041.png" alt="image-20210316134312041" style="zoom:67%;" />

* 数据层：Data Object：数据模型。借助MyBatis的ORMapping的操作，将关系型数据库的表结构通过xml的方式，定义成Java对应的Object的对象，因此数据层的Data Object就可以同数据库映射，用以ORM的方式操作数据库

* 业务层：Domain Model：领域模型。

  * 拥有完整的生命周期，有领域层的对象的创建、更新、删除、消亡的过程。数据层的模型更多关注数据库的ORMapping的操作，但是业务层的领域模型可以和Data Object做组合的关联关系。
  * 如用户对象，用户领域模型由对应用户基本信息和账户信息(密码)组成，但是数据库层面，设计成用户基础表和用户密码表，也就是说一个领域模型对应了两个Data Object数据模型。因为除了修改密码等操作，一般的业务逻辑不涉及密码，只需要用到用户的基本信息。
  * 领域模型是一个贫血模型，即对应的Domain Model只有自己的属性以及get()和set()方法，不提供相关的服务，用户要使用相关的服务，需要通过SprintBoot的Service服务。

  * 需要最先设计。

* 接入层模型：View Object，与前端对接的模型，前端展示的聚合模型，隐藏内部实现。根据前端需要来聚合领域模型。

![image-20210316135934570](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316135934570.png)



#### 2. 项目结构

##### 2.1 项目管理

使用SpringBoot + Maven的方式管理相关包和依赖

```properties
<parent>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-parent</artifactId>
    <version>2.0.5.RELEASE</version>
</parent>
    
<dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-web</artifactId>
</dependency>
```



##### 2.2 mybatis-generator

使用mybatis-generator.xml生成对应的Data Object。使用`com.mysql.jdbc.Driver`来连接数据库

* 生成对应表和类

  ```java
  <table tableName="promo"  domainObjectName="PromoDO" enableCountByExample="false"
                 enableUpdateByExample="false" enableDeleteByExample="false"
                 enableSelectByExample="false" selectByExampleQueryId="false"></table>
  ```

* 可以快速生成mapper文件和Data Object的相关文件

##### 2.3 配置文件

application.properties

```properties
server.port=8090
mybatis.mapperLocations=classpath:mapping/*.xml

spring.datasource.name=seckill
spring.datasource.url=jdbc:mysql://127.0.0.1:3306/seckill?useUnicode=true&characterEncoding=UTF-8&useSSL=false
spring.datasource.username=root
spring.datasource.password=root

#使用druid数据源
spring.datasource.type=com.alibaba.druid.pool.DruidDataSource
spring.datasource.driverClassName=com.mysql.jdbc.Driver

spring.mvc.throw-exception-if-no-handler-found=true
spring.resources.add-mappings=false
```

##### 2.4 数据库表

* user_info：用户基本信息表

* user_password：用户密码表，通过user_id与user_info表相关联

* item：商品信息表

* item_stock：商品库存表。后面需要对库存进行优化；库存操作非常耗性能，每次减库存都需要加行锁，日后对库存进行大变动时，可以将item_stock拆到另一个数据库中；也可以对item_stock进行分库分表，减少性能消耗

  索引：item_id

* promo：秒杀信息。活动id，开始时间，结束时间等

* order_info：记录交易单号。没有采用主键自增，采用sequence的方式。由sequence_info表提供了order_info表的主键的sequence(当前值value以及步长step)。单号"交易时间 + sequence + 库编号"

* sequence_info：提供sequence序列号。value，step

##### 2.5 项目目录

*<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316143818941.png" alt="image-20210316143818941" style="zoom:50%;" />*

* mapping：根据数据库设计，生成的对应的mapper文件，用于操作Data Object，包括映射以及select、insert、update、delete等操作
* controller：相关controller的操作，并返回View Object
* service：定义相关服务的接口和实现类，输出领域模型(model)
* dataobject、dao： DO层。与数据库一一映射的xxxDO类；dao中的接口与mapper中的操作对应(自动生成)
* validator：校验数据
* error：包装通用异常

##### 2.6 类图

![image-20210316144319803](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210316144319803.png)

#### 3. 实现逻辑

##### 3.0 异常

* 定义了通用的BusinessException，在该类中定义了commonError，commonError定义了getErrCode()、getErrMsg()、setErrMsg()方法。 

* 捕获到BusinessException异常后，将错误码ErrCode和ErrMsg放入responseData中，将CommonReturnType输出给前端

* CommonReturnType：包含 status 和 data 两部分。若status = "success"，data返回给前端需要的json数据；若status = "fail"，data返回对应的错误码格式

* 通用错误使用枚举类定义

  ```java
  //通用错误类型10001
      PARAMETER_VALIDATION_ERROR(10001,"参数不合法"),
      UNKNOWN_ERROR(10002,"未知错误"),
  
      //20000开头为用户信息相关错误定义
      USER_NOT_EXIST(20001,"用户不存在"),
      USER_LOGIN_FAIL(20002,"用户手机号或密码不正确"),
      USER_NOT_LOGIN(20003,"用户还未登陆"),
      //30000开头为交易信息错误定义
      STOCK_NOT_ENOUGH(30001,"库存不足"),
      ;
  ```

  

##### 3.1 用户注册

* 用户输入手机号，点击获取验证码。通过getOtp()接口，获取用户注册的验证码，验证码使用Random方法生成6位随机数，将6位验证码和用户手机号存到当前Session中，并将验证码通过短信通道发给用户(暂时通过命令台输出)。
* 用户填写手机、验证码以及相关基本信息，点击注册。register方法接收form表单上的基本信息，首先通过getSession()验证手机号与验证码是否相符。验证成功，将基本信息赋值到userModel(领域模型对象)中，密码通过Md5加密，然后调用**service**层的register(userModel)方法。
* service层的register方法：**@Transctional：将注册流程放在一个事务中，注册流程中任意一步失败，会进行回滚；**使用validate对userModel中的属性进行验证，validate会得到触发了验证规范的错误信息(在userModel属性上加@NotBlank、@NotNull等注解)，并返回。
* 通过验证后，将userModel转为对应的data object，得到userDO后，调用userDOMapper的insertSelective(userDO)，同时会捕获手机号重复的异常。
* 将userModel转换为 userPassword 的 data object

##### 3.2 登录

* validateLogin方法，通过telephone拿到userDO，进而到密码表拿到对应的用户密码，组装成userModel，比对数据库中的信息与对应UI传来的密码是否匹配
* 若比对成功，将登录成功的状态加入到Session中，并将用户信息userModel存入Session中，并返回通用返回对象CommonReturnType



##### 3.3 商品列表浏览

* 通过**service**层的listItem方法，调用itemDOMapper的listItem方法，调用do层的方法，得到所有的itemDO，并转换为itemModelList，这里要查两个表，商品表和库存表。将itemDO和itemStockDO通过convert方法转换为itemModel，将结果返回到**controller**层
* controller层将service层传来的itemModel转换为itemDO，过程中需要判断该商品是否存在秒杀活动，将结果返回给前端



##### 3.4 商品详情&秒杀活动

* 从商品基础表获取商品item_id，从库存表获取库存数量，合并为itemModel；根据promo_id判断是否在活动中
* 根据promo_id获取活动信息promoModel；promoModel中的status(1：活动未开始，2：进行中，3：已结束)
* 根据status判断当前时间秒杀活动是否即将开始或者正在进行，如果是即将开始或正在进行，则将promoModel合并到itemModel中，最后将itemModel转化为itemVO，一起返回给前端。
* 下单首先校验下单状态、下单商品是否存在、用户是否合法、购买数量是否正确；然后校验活动信息，校验对应活动是否存在这个适用商品，校验活动是否正在进行中；订单流水号：当前日期 + sequence拼接得到



##### 3.5 项目总结

* 在数据库设计时，字段要设置not Null，并设置默认值，避免唯一索引在null情况下失效等场景

* 使用MyBatis自动生成相关文件，配置mybatis-generator.xml文件，生成xxxDOMapper、dao、data object相关文件

* 调用获取验证码的 getotp.html时，遇到跨域问题

  * 解决方法：

    * 使用注解**`@CrossOrigin(allowCredentials = "true", allowedHeaders = "*")`**

      这个注解一加后，所有的http response头上都会加上
      Access-Control-Allow-Origin * 以及
      Access-Control-Allow-Headers * 两个头部，这样可以满足CORS的跨域定义，我们的Ajax看到这两个头部就认定对应的域名接收任何来自或不来自于本域的请求

    * 跨域和跨域传递cookie是两个不同纬度的问题，我们依靠上述的方式解决了跨域的问题，但是要做到跨域感知session需要解决在跨域的前提下将cookie也能传上去，这个时候就需要设置另外一个头部 ，我们的cross origin演变为

      **`@CrossOrigin(origins = {"*"},allowCredentials = "true",allowedHeaders = "*")`**

      使用了allowCredentials后Access-Control-Allow-Credentials头被设置成true，同时前端设置**`xhrField:{withCredential:true}`**后，浏览器在Ajax请求内带上对应的cookie头部和后端的allowCredentials配合在一起解决跨域传递cookie的问题。由于课程中仅仅使用了get和post的方法，而这两个方法在跨域请求中都是可以用的，因此allowedHeaders可以不加。

      另外当设置了allowCredentials = “true"的时候origins = {”*"}就失效了，因为一旦设置了跨域传递cookie就不能再设置接受任何origins了，而SprintBoot的实现方式是返回的allow origin取request内的origin即我们自己对应的HTML页面的路径。这样就可以做到在哪个origin上使用跨域就允许哪个origin，一样能达到我们想要的效果。

      PS：许多浏览器包括safari和最新版本的chrome默认设置都是不支持携带跨域cookie的，即便我们代码写成允许，浏览器底层也做了限制，因此在调试的时候我们可以关闭对应的限制，也可以使用扩展阅读内的其他跨域处理方式

* 使用同一的返回格式CommonReturnType，包括`status`和`data`

* dataobject：与数据库对应的映射对象Model

  Domain Model： service层用于业务逻辑的Model

  View Model：用于返回给前端的Model

* 使用 hibernate-validator 通过注解来完成模型参数校验

* 如果数据库中的主键id是自增的，在对应的DOMapper.xml中的insertSelective中需设置`keyProperty="id" useGeneratedKeys="true"`

* 注解：方法A中调用B，再调用C，单个方法均有插入数据，查询数据

  * 方法B注解：**`@Transactional(propagation = Propagation.REQUIRES_NEW)`**，不管是否存在事务，都创建一个新的事务，原来的事务挂起，新的执行完毕后继续执行原来的事务

  * 方法C注解：**`@Transactional(propagation=Propagation.REQUIRED) `**，如果有事务,那么加入事务,没有的话新建一个
  * 当B或C报错是，ABC均回滚
  * 当ABC异常被捕获时，ABC都不回滚
  * 当BC执行后，A报错时，AC回滚，B不回滚（B数据入库）

* itemModel使用了聚合模型，包括了PromoModel promoModel，如果为空，表示当前商品不在任何秒杀活动中；若不为空表示其有未结束的秒杀活动；在orderModel中加入promoId，若不为空，则以秒杀方式下单

* spring.mvc.throw-exception-if-no-handler-found=true,由于SprintBoot的mvc机制默认对于找不到handler，也就是找不到路径处理controller的方法时会使用404的错误交给servlet默认去处理，因此需要设置成true使其可以抛出异常，这样才能被我们定义的全局异常处理捕获到。

  其次 spring.resources.add-mappings表示我们不要开启默认的静态资源处理机制，而使用我们自己定义的静态资源处理的resourcesHandler。因此你需要手动添加静态资源处理的handler，例如像代码中一样，使用自定义的的/static/**的路径指向你jar包classpath下的路径已使得其可以找到静态资源



#### 4. 项目补充

##### 4.1 加密

* 哈希散列：使用哈希算法，如MD5

* 对称加密：通信双方约定一个一样的对称加密密钥，密钥存在客户端，有安全隐患

* 非对称加密：RSA算法，通过数学公式生成“公钥”，“私钥”对。1. 客户端在连接建立之初就获取到服务器端的公钥。2. 客户端要传输数据前使用公钥加密传输内容。 3. 服务端获取加密后的数据并用自己的私钥解密获取传输内容。

  公钥是公共的。私钥是绝密的，由服务端保密持有。

##### 4.2 跨域问题

* 原因：使用ajax请求；浏览器的限制；访问的域名不同
* 解决方法
  * 使用jsonp，内部使用<img><script>等不受限制的标签，不通过ajax访问服务器
  * 设置服务端的http response，SpringBoot的@CrossOrigin注解
  * 通过nginx反向代理设置httpresponse
  * 通过nginx将对应的服务部署在不同的机器上，然后使用公共的域名作为nginx反向代理的入口域名，使不同源变成同源



#### 项目未来优化：

* 容量问题
* 系统水平扩展
* 查询效率低下
* 活动开始前页面被疯狂刷新
* 库存行锁问题
* 下单操作多，缓慢
* 浪涌流量如何解决



#### 5. 项目部署阿里云

##### 5.1 环境配置

* Java 环境安装

* mysql环境安装 -> 本地数据库表的备份、服务器上恢复

  `systemctl start mariadb.service`

* 项目打包

  * 打包时需要添加，这样才能将相关的依赖一起打包

    ```java
          <plugin>
              <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-maven-plugin</artifactId>
          </plugin>
    ```

    

  * 问题：打包时报错`No compiler is provided in this environment. Perhaps you are running on a JRE`

  * 解决：配置环境变量使用JAVA_HOME(貌似没用)；在maven安装目录下的bin中的mvn.cmd文件中在第一行加`set JAVA_HOME=D:\xxx\xxx`

* 启动脚本

  * nohup：命令行退出，进程也不会退出

    ```shell
    nohup java -Xms400m -Xmx400m -XX:NewSize=200m -XX:MaxNewSize=200m -jar miaosha.jar --spring.config.addtion-location=/var/www/miaosha/application.properties
    ```



#### 6. 性能压测

##### 6.1 jmeter压测

* 线程组
* HTTP请求
* 查看结果树
* 聚合报告（平均、中位数、90%、95%、tps等）

* 查看应用线程数：·`pstree -p 2740 | wc -l`



##### 6.2 尝试压测

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210317165912308.png" alt="image-20210317165912308" style="zoom: 67%;" />

* 

* 压测发现容量问题：server端并发线程数上不去

  * spring-configuration-metadata.json文件下

    <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210317190706696.png" alt="image-20210317190706696" style="zoom:67%;" />

  * 调整默认参数：

    ```shell
    server.tomcat.accept-count=1000
    server.tomcat.max-threads=200
    server.tomcat.min-spare-threads=100
    ```

    TPS：每秒传输的事物处理个数

    **应用没有做任何优化时，单台机器只有200TPS的查询量**，显然不大行

    

##### 6.3 定制化内嵌TomCat开发

* keepAliveTimeOut：多少毫秒后不响应，断开keepalive
* maxKeepAliveRequests：多少次请求后Keepalive断开失效
* 使用WebServerFactoryCustomizer<ConfigurableServletWebServerFactory>定制化内嵌tomcat配置

> SpringBoot启动之后，会将application.properties中Tomcat相关的参数加载到protocol内，我们定义类一个WebServerConfiguration，他会将我们组装的参数传到ConfigurableWebServerFactory内，factory内可以将参数customize出来，最后可以对参数进行一次修改

##### 6.4 容量问题（优化方向）

* 响应时间长，TPS上不去

* 单Web容器上限

  * 线程数量：4核CPU 8G内存单进程调度线程数800-1000以上后会花费巨大的时间在CPU调度上
  * 等待队列长度：队列做缓冲池用，但不能无限长，太长会消耗内存，入队出队也会消耗CPU

* 因此，存在一个线程数的**拐点**

* MySQL数据库QPS(Queries Per Second 每秒查询数)容量问题

  * 主键查询：千万级别数据 -> 1-10毫秒
  * 唯一索引查询：千万级别数据 -> 10-100毫秒
  * 非唯一索引查询：千万级别数据 -> 100-1000毫秒
  * 无索引：百万条数据 -> 1000毫秒 + （不可接受！）

  因此，尽可能在主键和唯一索引上查询。

* MySQL数据库TPS(每秒事务数)容量问题

  * 非插入更新删除操作：同查询
  * 插入操作：1w ~10w tps（依赖配置优化）
    * 使用批量插入的sql语句，而不要用for循环逐个插入
    * 使用事务包裹所有的插入语句，而不要每一个查询都起一个事务
    * 插入的时候尽量保证插入的条目顺序是按照索引的递增顺序插入的，这样可以避免频繁的调整索引
    * 插入的总数据量大小不要小于innodb_log_buffer_size这个选项的大小，超过这个大小则会发生频繁的磁盘内存切换。可以分批插入事务提交

#### 7. 分布式扩展

##### 7.0 准备工作

* 单机容量问题，水平扩展
  * 表现：单机CPU使用率高，memory占用增加，网络带宽使用增加
  * cpu us：用户空间的 cpu 使用情况（用户层代码）
  * cpu sy：内核空间的 cpu 使用情况（系统调用）
  * load average： 1, 5, 15分钟 load 平均值，跟着核数系数，0代表通常，1代表打满，1+ 代表等待阻塞
  * memory： free空闲内存，used使用内存



* 修改部署结构

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210317212920773.png" alt="image-20210317212920773" style="zoom:50%;" />

  * 数据库远程开放端口连接

    * 在MySQL的mysql库里的user表中设置权限：

      ```
      use mysql;
      select host,user,password from user;
      grant all privileges on *.* to root@'%' identified by 'password';
      flush privileges;
      select host,user,password from user;
      ```
      
      

##### 7.1 nginx反向代理

* 安装OpenResty，需要先安装pcre, openssl, gcc, curl等
* 解压
* ./configure
* make
* make install
* nginx默认安装在//user/local/openresty/nginx目录

###### 7.1.1 nginx web服务器

* location节点path：指定url映射key
* location节点内容：root指定location path后对应的根路径，index指定默认的访问页
* sbin/nginx -c conf/nginx.com启动
* 修改配置后直接sbin/nginx -s reload无缝重启
  * 不影响用户的连接，但改变了进程号

###### 7.1.2 nginx动静分离服务器

* location节点path特定resources：静态资源路径
  * 进入nginx根目录的html下，新建resources目录存放所有前端静态资源
* location节点其他路径：动态资源路径

###### 7.1.2 nginx动态请求反向代理

* 设置upstream server

  ```
  upstream backend_server{
      server 172.21.168.92 weight=1;
      server 172.21.168.93 weight=1;
  }
  ```

  > 这里的weight表示服务器后端的轮询采用1:1

* 设置动态请求`location`为proxy pass路径

  ```
  location / {
      proxy_pass http://backend_server;
      proxy_set_header Host $http_host:$proxy_port;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
  }
  ```


###### 7.1.3 开启tomcat access log验证

```
server.tomcat.accesslog.enabled=true
server.tomcat.accesslog.directory=/var/www/miaosha/tomcat
server.tomcat.accesslog.patten=%h %l %u %t "%r" %s %b %D
```

​	%h：远端的host，ip地址

​	%u：远端主机的user

​	%t：处理时长

​	%r：请求方法，请求url

​	%s： http请求状态码

​	%b：请求response的大小

​	%D：处理请求的时长

###### 7.1.4 nginx反向代理后压测

* 2cpu 8GB，带宽20M：大概1600TPS，平均响应400毫秒以内



###### 7.1.5 nginx高性能原因（重要）

* epoll多路复用

  解决IO阻塞问题

  多路复用：在Linux中，所有的设备操作都是操作文件，所以每个socket连接，相当于操作一个socketfd。在同步阻塞的IO中，为了提高并发度，会为每个socket连接分配一个线程去处理。这样在高并发的情况下，导致系统存在大量的线程，从而使可用性下降。为了解决这个问题，Linux底层把那些建立起连接的socketfd,提交给系统底层的epoll进行管理。每次系统调用的时候，只会把那些已经发生某些事件的socket连接批量发送给客户端，在这种情况下只需要用一个线程去监控所有socket可能存在的状态。把那些已经发生事件的socket提取出来，放到一个list集合中，批量发送给客户端。客户端根据socket的具体事件，去做相应的处理。
  
  > - - -
  >Linux底层epoll的3个实现函数：
  > 
  > int epoll_create(int size);
  >int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
  > int epoll_wait(int epfd, struct epoll_event *events,int maxevents, int timeout);
  > epoll_create：创建一个epoll对象。参数size是内核保证能处理最大的文件句柄数，在socket编程里面就是处理的最大连接数。返回的int代表当前的句柄指针，当然创建一个epoll对象的时候，也会相应的消耗一个fd，所以在使用完成的时候，一定要关闭，不然会耗费大量的文件句柄资源。
  > 
  > epoll_ctl：可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等。其中epfd，就是创建的文件句柄指针，op是要做的操作，例如删除，更新等，event 就是我们需要监控的事件。
  >
  > epoll_wait：在调用时，在给定的timeout时间内，当在监控的所有句柄中有事件发生时，就返回用户态的进程。
  >
  > epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄发送给用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
  >
  > 那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。（当网卡里面有数据的时候，会发起硬件中断，提醒内核有数据到来可以拷贝数据。当网卡通知内核有数据的时候，会产生一个回调函数，这个回调函数是epoll_ctl创建的时候，向内核里面注册的。回调函数会把当前有数据的socket（文件句柄）取出，放到list列表中。这样就可以把存放着数据的socket发送给用户态，减少遍历的时间，和数据的拷贝）
  > 
  > - - -
  > 
  >参考1：https://blog.csdn.net/u014730165/article/details/85044285
  > 
  >参考2-epoll实现原理：https://blog.csdn.net/u014730165/article/details/85089085

  

  * java BIO模型，阻塞进程式

    > 阻塞IO：调用者必须等到方法调用返回后，才能继续后续的行为。
    >
    > 非阻塞IO：客户端隔段时间去看下有没有数据完成，那么这个过程中，中间间隔的时间，就可以去做一下自己想做的事情。当然整个过程中，服务端还是同步的，即同步非阻塞。

  * linux select模型，变更触发轮询查找，有1024数量上限

  * epoll模型，变更触发回调直接读取，理论无上限

    <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318084823738.png" alt="image-20210318084823738" style="zoom:67%;" />
    
    

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318085705423.png" alt="image-20210318085705423" style="zoom:67%;" />

  

* master worker进程模型

  平滑过渡，平滑重启，结合epoll多路复用，worker单线程模型，高效地完成操作

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318092232950.png" alt="image-20210318092232950" style="zoom:67%;" />

  > master进程用来管理worker进程，worker进程用来处理客户端连接。当启动master进程后，会创建一个socket文件句柄，用于监听在80端口上，即nginx的web服务端口，这时master就会启用epoll的多路复用模型，然后当client发送http请求，进行TCP三次握手，会向80端口发送socket的connect操作，对应的epoll的模型会产生一个回调，告诉worker进程，让worker进程去处理connect操作，而master进程不会处理connect操作。
  >
  > 那到底哪个worker去处理呢？nginx在内存中使用了一个accept mutex(互斥锁)，内存锁，因为主子进程是共享内存的，所以worker进程会去accept mutex复制体上抢占对应的锁，谁先抢占到锁，谁就有权限去处理connect，并且以后所有该client的连接请求，都由这个worker进程去负责。
  >
  > * 修改配置后 重启的指令：sbin/nginx -s reload  该指令不会改变master进程的进程号，会改变worker进程的进程号，并且所有的socket不会受影响。
  >
  >   原因：master进程会让worker进程把所有的socket的句柄交给master进程管理，然后master进程会重新new出新的worker进程，并将对应的句柄交给新的worker进程去管理，这些操作都是在内存操作
  >
  > * 这里每个worker进程只有一个线程

* 协程机制

  将每个用户的请求对应到线程中的一个协程，在协程中使用epoll多路复用机制，完成对应的同步调用开发，高效

  * 依附于线程的内存模型，切换开销小(1个线程可以有多个协程)
  * 遇阻塞及归还执行权，代码同步
  * 无需加锁

##### 7.2 会话管理

* 会话管理
  * 基于cookie传输sessionid： java tomcat容器session实现
  * 基于token传输类似sessionid： java代码session实现

* 分布式会话

  * 基于cookie传输sessionid： java tomcat容器session实现迁移到Redis

    * 为什么需要分布式session？之前的session信息是存在tomcat的内嵌容器中的，如果一个用户在服务器A完成登录操作，但下一次请求发送到了服务器B上，那他对应的登录信息就不存在了

    * 要存入Redis的数据，必须实现Serializable接口；或者修改Redis默认的序列化方式，使用Json

  * 基于token传输类似sessionid： java代码session实现迁移到Redis

    ```java
    @Autowired
    private RedisTemplate redisTemplate;
    
    // 修改成若用户登录验证成功后，将对应的登录信息和登录凭证一起存入redis中
    // 生成登录凭证token，UUID，全局唯一，32位
    String uuidToken = UUID.randomUUID().toString();
    
    // 建立token和用户登录态之间的联系
    redisTemplate.opsForValue().set(uuidToken, userModel);
    // token超时时间
    redisTemplate.expire(uuidToken, 1, TimeUnit.HOURS);
    

// 下发了token
    return CommonReturnType.create(uuidToken);

    前端：使用localStorage
    
    ```css
    success:function(data){
    					if(data.status == "success"){
    						alert("登陆成功");
    						var token = data.data;
    						window.localStorage["token"] = token;
    						window.location.href="listitem.html";
    					}else{
    						alert("登陆失败，原因为"+data.data.errMsg);
    				}
    				},
```

    下单逻辑修改：
    
    ```java
    String token = httpServletRequest.getParameterMap().get("token")[0];
    if (StringUtils.isEmpty(token)) {
        throw new BusinessException(EmBusinessError.USER_NOT_LOGIN, "用户还未登陆，不能下单");
    }
    UserModel userModel = (UserModel)redisTemplate.opsForValue().get(token);
    //获取用户的登陆信息
    //        UserModel userModel = (UserModel) httpServletRequest.getSession().getAttribute("LOGIN_USER");
    if (userModel == null) {
        throw new BusinessException(EmBusinessError.USER_NOT_LOGIN, "用户还未登陆，不能下单");
    }
    OrderModel orderModel = orderService.createOrder(userModel.getId(), itemId, promoId, amount);
      
    return CommonReturnType.create(null);
```

##### 7.3 一些问题

* Nginx容器作为反向代理的中间节点，是如何保证路由策略的性能高效的？
  
  使用HTTP1.1协议长连接KeepAlive，KeepAlive可以解决网络建连问题，同时nginx的进程模型可以保证非阻塞模式的高性能运行
  
  * 数据库如何进行水平扩展？
  
    > 数据库扩展首先是读写分离，一主多从，主从之间使用弱一致性同步数据以保证性能，这样应用就会面临读过期数据的问题，需要在业务层上做折中，若业务需要读到强一致数据就读主库，主库可以采取keepalive方式同另一个不开放的主库做同步，但为了性能考虑一般不对两个主库做双写，发生问题切换的时候需要应用层做好failover机制。
    >
    > 然后是分库分表，对于join，orderby这种尽可能使得其可以路由在同一个分片区保证性能，否则等于跨数据库查询，性能很差，若无法做到路由位控制的就存储冗余数据，分布式事务的话是另一个话题，有多种方式保证。
    >
    > 无论以上哪种处理方案，都记住cap理论，也就是分布式的情况下可用性和强一致性只能选其一，架构师不要花心思在中间件怎么又能保证c又能保证a上，而应该考虑在出现问题的时候怎么恢复数据，或者索性就跟hbase一样作强一致性数据库。

#### 8. 查询优化技术之多级缓存

##### 8.1 缓存设计

* 用快速存取设备，用内存
* 将缓存推到离用户最近的地方
* 脏缓存清理

##### 8.2 多级缓存

* redis缓存
* 热点内存本地缓存
* nginx proxy cache缓存
* nginx lua缓存

##### 8.3 redis缓存

* 单机版
* sentinal哨兵模式

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318155049488.png" alt="image-20210318155049488" style="zoom: 50%;" />

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318155143515.png" alt="image-20210318155143515" style="zoom:50%;" />



* 集群cluster模式

  > 不需要关心谁是master，访问任意一台redis就能得到redis集群的信息；一旦有一台redis挂了，redis会返回一个reask，来重新获取集群列表信息

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318155845657.png" alt="image-20210318155845657" style="zoom: 67%;" />



<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318155956388.png" alt="image-20210318155956388" style="zoom:67%;" />

##### 8.4 商品详情动态内容实现

```java
// 商品详情页浏览
@RequestMapping(value = "/get", method = {RequestMethod.GET})
@ResponseBody
public CommonReturnType getItem(@RequestParam(name = "id") Integer id) {
    ItemModel itemModel = null;
    // 先取本地缓存
    itemModel = (ItemModel) cacheService.getFromCommonCache("item_" + id);
    if (itemModel == null) {
        // 根据商品的id到redis内获取
        itemModel = (ItemModel) redisTemplate.opsForValue().get("item_" + id);
        // 若redis内不存在对应的ItemModel，则访问下游service
        if (itemModel == null) {
            itemModel = itemService.getItemById(id);
            // 设置ItemModel到redis内
            redisTemplate.opsForValue().set("item_" + id, itemModel);
            // 暂时设置10分钟失效时间
            redisTemplate.expire("item_" + id, 10, TimeUnit.MINUTES);
        }
        // 填充本地缓存
        cacheService.setCommonCache("item_" + id, itemModel);
    }
    ItemVO itemVO = convertVOFromModel(itemModel);
    return CommonReturnType.create(itemVO);
}
  ```

问题：这时候查看redis数据库，里面的key和value是乱码。因此需要配置RedisConfig修改redis的序列化方式

​```java
@Component
@EnableRedisHttpSession(maxInactiveIntervalInSeconds = 3600)
public class RedisConfig {
    @Bean
    public RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) {
        RedisTemplate redisTemplate = new RedisTemplate();
        redisTemplate.setConnectionFactory(redisConnectionFactory);

        // 首先解决redis中key的序列化方式
        // 将原本java的序列化方式，改为string的序列化方式
        StringRedisSerializer stringRedisSerializer = new StringRedisSerializer();
        redisTemplate.setKeySerializer(stringRedisSerializer);

        // 解决value的序列化方式
        // 一般使用JSON序列化方式
        Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);

        ObjectMapper objectMapper = new ObjectMapper();
        SimpleModule simpleModule = new SimpleModule();
        simpleModule.addSerializer(DateTime.class, new JodaDateTimeJsonSerializer());
        simpleModule.addDeserializer(DateTime.class, new JodaDateTimeJsonDeserializer());

        objectMapper.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL);

        objectMapper.registerModule(simpleModule);
        jackson2JsonRedisSerializer.setObjectMapper(objectMapper);

        redisTemplate.setValueSerializer(jackson2JsonRedisSerializer);

        return redisTemplate;
    }
}
  ```





##### 8.5 性能压测

> 1000线程，循环20，2核8GB，20Mbps带宽： TPS： 1900+，平均延时300-毫秒
>
> cpu：占用也很低，



##### 8.6 本地热点缓存

* 热点数据
* 脏读不敏感
* 内存可控



###### 8.6.1 Guava cache

* 可控的大小和超时时间
* 可配置的LRU(最近最少使用)策略
* 线程安全

```java
package com.qy.miaoshaproject.service;

/**
 * 封装本地缓存操作类
 */
public interface CacheService {
    // 存方法
    void setCommonCache(String key, Object value);

    // 取方法
    Object getFromCommonCache(String key);
}
```

```java
package com.qy.miaoshaproject.service.impl;

import com.google.common.cache.Cache;
import com.google.common.cache.CacheBuilder;
import com.qy.miaoshaproject.service.CacheService;
import org.springframework.stereotype.Service;

import javax.annotation.PostConstruct;
import java.util.concurrent.TimeUnit;

/**
 * @Description: TODO
 * @author: lqy
 * @date: 2021/03/18/ 20:12
 */
@Service
public class CacheServiceImpl implements CacheService {

    private Cache<String, Object> commonCache = null;

    @PostConstruct
    public void init() {
        commonCache = CacheBuilder.newBuilder()
                // 设置缓存容器的初始容量为10
                .initialCapacity(10)
                // 设置缓存中最大可以存储100个key，超过100个之后会按照LRU策略移除缓存项
                .maximumSize(100)
                // 设置写缓存后多少秒过期
                .expireAfterWrite(60, TimeUnit.SECONDS).build();
    }

    @Override
    public void setCommonCache(String key, Object value) {
        commonCache.put(key, value);
    }

    @Override
    public Object getFromCommonCache(String key) {
        return commonCache.getIfPresent(key);
    }
}
```



优化本地缓存后压测：1000线程，循环20，2核8GB，20Mbps带宽： TPS： 3000+，平均延时160-毫秒

###### 8.6.2 nginx proxy cache缓存

* nginx反向代理
* 依靠文件系统存索引级的文件
* 依靠内存缓存文件地址

```
proxy_cache_path /usr/local/openresty/nginx/tmp_cache levels=1:2 keys_zone=tmp_cache:100m inactive=7d max_size=10g;
```

> 压测后，效果不如Guava cache

###### 8.6.3 nginx lua

* lua协程机制：线程空间内的执行单元，有自己独立的运行空间，依托于线程的内存模型，切换开销非常小；遇到阻塞及归还执行权，代码可以以完成同步的方式去模拟异步调用；协程在线程中一般是串行执行，因此无需加锁

  ```lua
  function foo(a)
      print("foo 函数输出", a)
      return coroutine.yield(2 * a) -- 返回 2 * a的值
  end
  
  co = coroutine.create(function(a, b)
     print("第一次协同程序执行输出", a, b) -- co-body 1 10 
     local r = foo(a + 1)
          
     print("第二次协同程序执行输出", r)
     local r, s = coroutine.yield(a + b, a - b) --a, b的值为第一次调用协同程序时传入
          
     print("第三次协同程序执行输出", r, s)
     return b, "结束协同程序"            -- b的值为第二次调用协同程序时传入
  end)
  
  print("main", coroutine.resume(co, 1, 10)) --true, 4
  print("--分割线---")
  print("main", coroutine.resume(co, "r")) --true, 11 -9
  print("--分割线---")
  print("main", coroutine.resume(co, "x", "y")) --true, 10 end
  print("--分割线---")
  print("main", coroutine.resume(co, "x", "y")) --cannot resume dead coroutine
  print("--分割线---")
  ```

* nginx协程

  * nginx的每一个Worker进程都是在epoll或kqueue这种事件模型之上，封装成协议
  * 每一个请求都由一个协程进行处理
  * 即使ngx_lua必须要运行Lua，相对C有一定的开销，但依旧能保证高并发能力

* nginx协程机制

  * nginx每个工作进程创建一个lua虚拟机
  * 工作进程内的所有协程共享一个vm
  * **每个外部请求由一个lua协程处理**，之间数据隔离
  * lua代码调用io等异步接口时，协程被挂起，上下文数据保持不变
  * 自动保存，不阻塞工作进程
  * io异步操作完成后还原协程上下文，代码继续执行

* nginx处理阶段

  ```
  NGX_HTTP_POST_READ_PHASE = 0 //读取请求头 
  NGX_HTTP_SERVER_REWRITE_PHASE  //执行rewrite -> rewrite_handler，动态请求和静态资源，如前面的static和ajax请求
  NGX_HTTP_FIND_CONFIG_PHASE //根据 uri 替换 location
  NGX_HTTP_REWRITE_PHASE //根据替换结果继续执行 rewrite -> rewrite_handler
  NGX_HTTP_POST_REWRITE_PHASE //执行 rewrite 后处理
  NGX_HTTP_PREACCESS_PHASE //认证预处理 请求限制，连接限制 -> limit_conn_hander limit_req_handler
  NGX_HTTP_ACCESS_PHASE //认证处理 -> auth_basic_handler,access_handler
  NGX_HTTP_POST_ACCESS_PHASE //认证后处理，认证不通过，丢包
  NGX_HTTP_TRY_FILES_PHASE , //尝试 try 标签
  NGX_HTTP_CONTENT_PHASE //内容处理(核心) -> static_handler
  NGX_HTTP_LOG_PHASE //日志处理 -> log_handler
  ```

* nginx lua插载点

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318222325290.png" alt="image-20210318222325290" style="zoom: 67%;" />

  * init_by_lua：系统启动时调用

  * init_worker_by_lua：worker进程启动时调用

  * set_by_lua： nginx变量用复杂lua return

  * rewrite_by_lua：重写url规则

  * access_by_lua：权限验证阶段

  * content_by_lua：内容输出节点

    **优点**：可以通过lua脚本，针对item/get等请求，在nginx上完成业务代码的处理逻辑，避免访问后端的java服务器

  

* OpenResty
  * 由Nginx核心加很多第三方模块组成，默认集成了Lua开发环境，使得Nginx可以作为一个Web Server使用
  * 借助于Nginx的事件驱动模型和非阻塞IO，可以实现高性能的Web服务器
  * 提供了大量组件如MySQL、Redis、Memcached等等，使在Nginx上开发Web应用更方便简单

###### 8.6.4 Openresty实践

* lua配置

  在RedisConfig中配置lua的文件位置

  ```
  lua_shared_dict my_cache 128m
      
  server {
      listen 80;
      server_name localhost;
      
      ...
      location /luaitem/get{
          default_type "application/json";
          content_by_lua_file ../lua/itemsharedic.lua;
      }
      ...
  }
  ```

* shared dic：共享内存字典，所有worker进程可见，LRU淘汰

  ```lua
  function get_from_cache(key)
          local cache_ngx = ngx.shared.my_cache
          local value = cache_ngx:get(key)
          return value
  end
  
  function set_to_cache(key,value,exptime)
          if not exptime then
                  exptime = 0
          end
          local cache_ngx = ngx.shared.my_cache
          local succ,err,forcible = cache_ngx:set(key,value,exptime)
          return succ
  end
  
  取到对应url中的参数
  local args = ngx.req.get_uri_args()
  local id = args["id"]
  local item_model = get_from_cache("item_"..id)  ..表示拼接
  if item_model == nil then
          local resp = ngx.location.capture("/item/get?id="..id)
          item_model = resp.body
          set_to_cache("item_"..id,item_model,1*60)
  end
  
  ngx.say(item_model)
  ```

* openresty redis支持

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210318231437502.png" alt="image-20210318231437502" style="zoom:67%;" />

  压测：200- 毫秒，TPS： 3000+

  

##### 8.7 思考

1. 如何解决缓存的脏读和失效的问题？

   缓存的脏读问题：也就是数据的不一致性。可以考虑对更新操作的时候先删除缓存，然后再更新数据库。

   缓存失效：可能会带来的缓存雪崩。可以考虑给每一个缓存的数据加上一个失效标记，若过期则启动另外一个线程进行缓存数据的更新。

2. 在大型的应用集群中若对Redis访问过度依赖，会否产生应用服务器到Redis之间的网络带宽产生瓶颈？若会产生瓶颈，如何解决这样的问题？

3. Nginx作为一个反向代理的中间件接入节点，若要感知到业务，是否会引入太多的定制化业务的能力，是否可以考虑隔离分层？多引入一层后又如何确保性能？

   可以考虑将nginx分为两层部署 第一层不管业务 只管代理 数据压缩等基础功能 第二层管业务 可以做缓存 限流等功能 做到区分 引入的代价是网络上多了一个消耗节点

#### 9. 静态资源cdn

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319081410845.png" alt="image-20210319081410845" style="zoom:67%;" />

![image-20210319090656559](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319090656559.png)

###### DNS

* DNS用CNAME解析到源站
* 回源缓存设置
  * cache control响应头
    * private：客户端可以缓存
    * public：客户端和代理服务器都可以缓存
    * max-age=xxx：缓存的内容将在xxx秒后失效
    * no-cache：强制向服务端再验证一次
      * 有效性判断
        * ETag：资源唯一标识
        * If-None-Match：客户端发送的匹配ETag标识符
        * Last-modified：资源最后被修改的时间
        * If-Modified-Since：客户端发送的匹配资源最后修改时间的标识符
    * no-store：不缓存请求的任何返回内容

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319083036906.png" alt="image-20210319083036906" style="zoom:67%;" />

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319083946747.png" alt="image-20210319083946747" style="zoom:67%;" />

###### 浏览器三种刷新方式 

* 回车刷新或a链接：看cache-control对应的max-age是否仍然有效，有效则直接from cache，若cache-control中为no-cache，则进入缓存协商逻辑
* F5刷新或command+R刷新：去掉cache-control中的max-age或直接设置max-age为0，然后进入缓存协商逻辑
* Ctrl+F5或command+shift+R刷新：去掉cache-control和协商头，强制刷新

> 协商机制：比较Last-modified和ETag到服务端，若服务端判断没变化则304不返回数据，否则返回200

###### CDN自定义缓存策略

* 可自定义目录过期时间

* 可自定义后缀名过期时间

* 可自定义对应权重

* 可以通过界面或api强制cdn对应目录刷新（非保成功）

  ![image-20210319084449808](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319084449808.png)

###### 静态资源部署策略

* css，js，img等元素使用带版本号部署，例如a.js?v=1.0不便利，且维护困难
* css，js，img等元素使用带摘要部署，例如a.js?v=45edw存在先部署html还是先部署资源的覆盖问题
* css，js，img等元素使用摘要做文件名部署，例如45edw.js，新老版本并存且可回滚，资源部署完后再部署html
* 对应静态资源保持生命周期内不会变，max-age可设置的很长，无视失效更新周期
* HTML文件设置no-cache或较短max-age，以便于更新
* HTML文件仍然设置较长的max-age，依靠动态的获取版本号请求发送到后端，异步下载最新的版本号的HTML后展示渲染在前端（异步更新）
* 动态请求也可以静态化成json资源推送到cdn上
* 依靠异步请求获取后端节点对应资源状态做紧急下架处理
* 可通过跑批紧急推送cdn内容以使其下架等操作

###### 全页面静态化

* 定义：在服务端完成html，css，甚至js的load渲染成纯HTML文件后直接以静态资源的方式部署到cdn上
* phantomjs：无头浏览器，可以借助其模拟webkit js的执行
  * 修改需要全页面静态化的实现，采用initView和hasInit方式防止多次初始化
  * 编写对应轮询生成内容方式
  * 将全静态化页面生成后推送到cdn

###### 思考

1. 使用全页面静态化技术之后，对应的缓存更新就变得非常困难，是否可以考虑将对应的商品模型做更细力度的拆分并使用不同的缓存策略？

   > 对于价格、库存实时性要求高的每次都到服务端拿最新的值；对于其他的属性取CDN内容就行，如果这些实时性要求不高的属性变化了，上游系统发给我们属性变更的消息，我们拿到这个消息触发脚本重新执行一遍无头js，然后把生成的html文件再推倒CDN

2. 全页面静态化技术如何在互联网环境下做到可及时下架 ?

   > 商品变动或下架后，由后端系统触发异步消息给某一个服务，这个服务负责调用爬虫重新生成最新的页面后推送给cdn服务，这些都是需要api对接的

#### 10. 交易优化技术之缓存库存

##### 10.1 交易性能瓶颈

* jmeter压测

  * 200线程：java服务器：500毫秒，300TPS，cpu：70%左右

    1000线程：数据库服务器：Cpu： 10%左右，耗时1800毫秒，450TPS

* 交易验证完全依赖数据库

* **库存行锁**

  所有减库存

* 后置处理逻辑

* 后台交易流程如图：

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319100351553.png" alt="image-20210319100351553" style="zoom:67%;" />

  

###### 10.1.1 交易验证优化

* 用户风控策略优化：策略缓存模型化
  * 可以将用户风控内容，通过异步的策略写入redis缓存中
* 活动校验策略优化：引入活动发布流程，模型缓存化，紧急下线能力
  * 可以将活动的信息放入缓存中；需要有紧急下线能力，一般活动发布需要在活动开始前半小时发布，后台可以提供紧急下线接口，通过代码清除缓存。

* jmeter压测：1000线程数：TPS： 1200+，600毫秒

##### 10.2 库存行锁优化

* 扣减库存缓存化

  * 方案：
    1. 活动发布同步库存进缓存
    2. 下单交易减缓存库存
  * 问题：
    1. 数据库记录不一致

* 异步同步数据库

  * 方案：

    1. 活动发布同步库存进缓存
    2. 下单交易减缓存库存
    3. **异步消息扣减数据库内库存**

  * **异步消息队列RocketMQ**

    * 高性能、高并发，分布式消息中间件

    * 典型应用场景：分布式事务，异步解耦

    *  概念模型

      ![image-20210319124717593](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319124717593.png)

    * 部署模型

      ![image-20210319125143015](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319125143015.png)

      ![image-20210319130412120](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210319130412120.png)

      一般主Broker去做生产/消费成功，从Broker采用异步复制的方式，这样效率和可用性比较高，但强一致性略低，有一定概率丢失部分消息

    * 分布式事务

      ![image-20210426170142040](D:%5CProgram%20Data%5CTypora%5Ctypora-user-images%5Cimage-20210426170142040.png)

      * ACID(原子性Atomicity、一致性Consistency、隔离性Isolation、持久性Durability)：刚性事务强一致

      * CAP(~~一致性Consistency~~、可用性`Availability`、分区容忍性`Partition tolerance`(必须))：牺牲瞬时强一致性，保证**基础可用性**和**最终一致性**

      * BASE(**基础可用性**Basically Available、软状态Soft-state、**最终一致性**Eventual Consistency)
      
        > 软状态：可以容忍瞬间存在的数据不一致

* 库存数据库最终一致性保证

* RocketMQ相关

  默认端口：9876

  RocketMQ要根据服务器的配置，设置默认的内存空间

  `mqnamesrv.xml`：新生代、永久代等都要改

  `runbroker.sh`：中也要改，Xms，Xmx等。

**代码实现**

实现一个MQProducer负责发送消息，实现一个MQConsumer负责接收消息，并实现真正的扣减库存操作。

首先配置`application.properties`

```java
mq.nameserver.addr=8.136.217.255:9876
mq.topicname=stock
```



* RocketMQ中遇到的问题

  * 创建Topic时`./mqadmin updateTopic -n localhost:9876 -t stock -c DefaultCluster`报错：

    ```
    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8
    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed i
    org.apache.rocketmq.tools.command.SubCommandException: UpdateTopicSubCommand command failed 
    at org.apache.rocketmq.tools.command.topic.UpdateTopicSubCommand.execute(UpdateTopicSubCom181)
    	at org.apache.rocketmq.tools.command.MQAdminStartup.main0(MQAdminStartup.java:135)
    	at org.apache.rocketmq.tools.command.MQAdminStartup.main(MQAdminStartup.java:86)
    Caused by: org.apache.rocketmq.acl.common.AclException: [10015:signature-failed] unable to calculast signature. error=[10015:signature-failed] unable to calculate a request signature. error=AlgoriA1 not available
    at org.apache.rocketmq.acl.common.AclSigner.signAndBase64Encode(AclSigner.java:84)
    at org.apache.rocketmq.acl.common.AclSigner.calSignature(AclSigner.java:73)
    at org.apache.rocketmq.acl.common.AclSigner.calSignature(AclSigner.java:68)
    at org.apache.rocketmq.acl.common.AclUtils.calSignature(AclUtils.java:58)
    ....
    
    ```

    解决方法：编辑rocket/bin下的tool.sh，将

    ```
    JAVA_OPT="${JAVA_OPT} -Djava.ext.dirs=${BASE_DIR}/lib:${JAVA_HOME}/jre/lib/ext"
    ```

    修改为

    ```
    JAVA_OPT="${JAVA_OPT} -Djava.ext.dirs=${BASE_DIR}/lib:${JAVA_HOME}/jre/lib/ext:/usr/java/jdk1.8.0_65/jre/lib/ext"
    ```

    * 调用时报错`closeChannel: close the connection to remote address[] result: true`

      解决：在服务端，要用公网ip，不能用localhost，并在conf/broker.conf 中 加入 brokerIP1=你的公网IP

      ```shell
      > nohup ./bin/mqnamesrv -n 你的公网IP:9876 &
      > nohup sh bin/mqbroker -n 你的公网IP:9876 -c conf/broker.conf autoCreateTopicEnable=true &
      ```

* 异步同步数据库
  * 问题
    * 异步消息发送失败
    * 扣减操作 执行失败
    * 下单失败无法正确回补库存

#### 11. 交易性能优化技术之事务型消息

上一章的代码处理逻辑

修改落单减库存的Service操作，从redis扣，然后发布异步消息给对应的库存的异步回调，在异步回调内扣减对应的数据库，如果异步消息发生了任何异常，将redis对应的库存加回去，或者redis库存更新失败，也吧对应的库存加回去。

**问题：**decreaseStack方法加了@Transactional注解，如果外部还有事务，那么会沿用外部的事务，要么同时成功，要么同时失败。因此如果后续操作订单入库发生异常，整个事务会进行回滚，那么我们的库存就白白消耗掉了，少卖了但是木有订单。

本质问题是分布式事务，在发送异步消息之前，我们不知道对应的减库存操作之后会不会失败

**处理**：增加异步更新库存操作，将原本减库存操作中的更新库存操作，放到外面，即返回前端前异步更新数据库。

**问题**：Spring Transactional的方法，只有最后返回成功后才会去commit。即使MQ发送成功，但是最后return失败，还是不行。

**处理：**

```java
// 最近的Transactional标签成功commit之后才会执行
TransactionSynchronizationManager.registerSynchronization(new 	TransactionSynchronizationAdapter() {
    @Override
    public void afterCommit() {
        // 异步更新库存
        boolean mqResult = itemService.asyncDecreaseStock(itemId, amount);
        // 这里mq消息一旦发生失败，就永远丢失掉了

        // if (!mqResult) {
            // 回滚redis库存
            //itemService.increaseStock(itemId, amount);
            //throw new BusinessException(EmBusinessError.MQ_SEND_FAIL);
        //}
    }
});
```

**问题：**MQ消息一旦发生失败，就永远丢失了

解决：使用RocketMQ的Transactional事务

一定要用事务型消息，也就是要保证数据库事务提交，那么对应的消息一定要发送成功；数据库内事务回滚了，消息一定不发送；数据库事务未提交，消息一定是等待状态。

`transactionMQProducer.setTransactionListener`会重写两个方法，`executeLocalTransaction`和`checkLocalTransaction`

`orderService.createOrder(userId, itemId, promoId, amount, stockLogId);`这里会向broker中投递一个prepare状态的消息。prepare状态下，消息对应的操作时不会被消费者看到的，这时客户端会去执行`executeLocalTransaction`方法，这个方法就是我们正真要做的事，可以理解为创建订单，也就是orderService的createOrder方法。

```java
@Component
public class MqProducer {

    private DefaultMQProducer producer;

    private TransactionMQProducer transactionMQProducer;

    @Autowired
    private OrderService orderService;

    @Autowired
    private StockLogDOMapper stockLogDOMapper;

    @Value("${mq.nameserver.addr}")
    private String nameAddr;

    @Value("${mq.topicname}")
    private String topicName;

    @PostConstruct
    public void init() throws MQClientException {
        // 做mq producer初始化
        transactionMQProducer = new TransactionMQProducer("transaction_producer_group");
        producer.setNamesrvAddr(nameAddr);
        producer.setSendMsgTimeout(10000);
        producer.start();

        transactionMQProducer.setTransactionListener(new TransactionListener() {
            @Override
            public LocalTransactionState executeLocalTransaction(Message message, Object args) {
                // 正真要做的事，创建订单
                Integer itemId = (Integer) ((Map) args).get("itemId");
                Integer promoId = (Integer) ((Map) args).get("promoId");
                Integer userId = (Integer) ((Map) args).get("userId");
                Integer amount = (Integer) ((Map) args).get("amount");
                String stockLogId = (String) ((Map) args).get("stockLogId");

                try {
                    orderService.createOrder(userId, itemId, promoId, amount, stockLogId);
                } catch (BusinessException e) {
                    e.printStackTrace();
                    // 设置对应的stockLog为回滚状态(3)
                    StockLogDO stockLogDO = stockLogDOMapper.selectByPrimaryKey(stockLogId);
                    stockLogDO.setStatus(3);
                    stockLogDOMapper.updateByPrimaryKeySelective(stockLogDO);
                    // 事务回滚
                    return LocalTransactionState.ROLLBACK_MESSAGE;
                }
                return LocalTransactionState.COMMIT_MESSAGE;
            }

            @Override
            // 可能executeLocalTransaction执行到一半断了，或者创建订单执行了很长时间
            public LocalTransactionState checkLocalTransaction(MessageExt messageExt) {
                // 根据是否扣减库存成功，来判断要返回COMMIT,ROLLBACK还是继续UNKNOWN
                String jsonString = new String(messageExt.getBody());
                Map<String, Object> map = JSON.parseObject(jsonString, Map.class);
                Integer itemId = (Integer) map.get("itemId");
                Integer amount = (Integer) map.get("amount");
                String stockLogId = (String) map.get("stockLogId");

                StockLogDO stockLogDO = stockLogDOMapper.selectByPrimaryKey(stockLogId);
                if (stockLogDO == null) {
                    return LocalTransactionState.UNKNOW;
                }
                if (stockLogDO.getStatus().intValue() == 2) {
                    return LocalTransactionState.COMMIT_MESSAGE;
                } else if (stockLogDO.getStatus().intValue() == 1) {
                    return LocalTransactionState.UNKNOW;
                }
                return LocalTransactionState.ROLLBACK_MESSAGE;
            }
        });
    }
```





##### RocketMQ事务型消息

###### 库存操作流水

数据类型：

* 主业务数据：master data
* 操作性数据：log data

###### 库存数据库最终一致性保证

* 引入库存操作流水
* 引入事务性消息机制

问题

* redis不可用时如何处理
* 扣减流水错误如何处理 

###### 业务场景决定高可用技术实现

* 设计原则，根据具体的业务场景情况而定
  * 宁可少卖，不能超卖

* 方案
  * redis可以比实际数据库少
  * 超时释放

##### 库存售罄

* 库存售罄标识
* 售罄后不去操作后续流程
* 售罄后通知各系统售罄(异步消息通知 )
* 回补上新

###### 后置流程

* 销量逻辑异步化

* 交易单逻辑异步化



#### 12. 流量削峰

解决的问题

* 秒杀下单接口会被脚本不停地刷
* 秒杀验证逻辑和秒杀下单接口强关联，代码冗余度高
* 秒杀验证逻辑复杂，对交易系统产生无关联负载

秒杀令牌

* 秒杀接口需要依靠令牌才能进入
* 秒杀令牌由秒杀活动模块负责
* 秒杀活动模块对秒杀令牌生成全权负责，逻辑收口
* 秒杀下单前需要先获得秒杀令牌

秒杀大闸原理

* 依靠秒杀令牌的授权原理定制化发牌逻辑，做到大闸功能
* 根据秒杀商品处理库存颁发对应数量令牌，控制大闸流量
* 用户风控策略前置到秒杀令牌发放中
* 库存售罄判断前置到秒杀令牌发放中

缺陷

* 浪涌流量涌入后系统无法应对
* 多库存，多商品等令牌限制能力弱

队列泄洪原理

* 排队有时候比高并发更高效(如redis单线程模型，InnoDB mutex key等)
* 依靠排队去限制并发流量
* 依靠排队和下游拥塞窗口程度调整队列释放流量大小

代码

在应用本地开一个20个线程的线程池，使用submit方法，实现Callable，将下单入库操作放在call方法中。相当于用一个拥塞窗口为20的等待队列，实现泄洪

```java
private ExecutorService executorService;

@PostConstruct
public void init() {
    // 开20个线程的线程池
    executorService = Executors.newFixedThreadPool(20);
}

// 同步调用线程池的submit方法
// 拥塞窗口为20的等待队列，用来队列化泄洪
Future<Object> future = executorService.submit(new Callable<Object>() {

    @Override
    public Object call() throws Exception {
        // 加入库存流水init状态
        String stockLogId = itemService.initStockLog(itemId, amount);

        // 完成对应的下单事务型消息机制
        if (!mqProducer.transactionAsyncReduceStock(userModel.getId(), itemId, promoId, amount, stockLogId)) {
            throw new BusinessException(EmBusinessError.UNKNOWN_ERROR, "下单失败");
        }
        return null;
    }
});
try {
    future.get();
} catch (InterruptedException e) {
    throw new BusinessException(EmBusinessError.UNKNOWN_ERROR);
} catch (ExecutionException e) {
    throw new BusinessException(EmBusinessError.UNKNOWN_ERROR);
}
```

本地和分布式

* 本地：将队列维护在本地内存中

  性能高，可用性高，可能会存在负载不均衡

* 分布式：将队列设置到外部redis内

  在外部redis内不可用时，可以切换到使用本地的队列



#### 限流

##### 限流方案

* 限制并发

* 令牌桶算法

* 漏桶算法

  <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210321154327818.png" alt="image-20210321154327818" style="zoom: 50%;" />

##### 限流力度

* 接口维度
* 总维度

##### 限流范围

* 集群限流：依赖redis或其他的中间件技术做统一计数器，往往会产生性能瓶颈
* 单机限流：负载均衡的前提下单机平均限流效果更好

#### 项目总结

##### 设计技术

* 分布式
  * nginx反向代理
  * 分布式会话管理
* 查询多级缓存
  * redis缓存
  * 本地缓存
  * 静态请求cdn
  * 动态请求缓存
  * 页面静态化
* 交易泄压
  * 秒杀令牌
  * 交易异步化
  * 异步化事务
* 流量错峰
  * 秒杀令牌
  * 秒杀大闸
  * 队列泄洪
* 防刷限流(未实现)
* 性能测试
  * jmeter压测

##### 项目框架

项目结构分层、业务逻辑分层、逻辑模型分层

##### 交易优化技术之缓存库存

交易验证：性能和正确性的权衡

库存模型：性能和可用性的权衡

##### 交易优化技术之事务型消息

ACID、CAP、BASE

最终一致性



压测：4核8GB、20M带宽

查询300到4000 交易100到700



#### 一些问题整理？

##### 如何优化的？

1. 先在**本地缓存**中找，本地缓存初始容量10，最大存储100个`key`，超过100个后使用`LRU`策略，缓存时效60秒；若本地缓存没有，则去`**redis`缓存**找，`redis`没有，则去调用下层服务，到数据库找，更新``redis``缓存，设置失效时间10分钟，更新本地缓存

2. 使用`nginx + lua + redis`实现**缓存预热**，使用`OpenResty`结合`lua`脚本，从数据库中读取商品信息，将商品信息保存到redis中，访问时，先查`OpenStack`本地缓存，如果没有，再查`redis`。

   优点：通过l`u`a脚本，针对`item/get`请求，在`nginx`上就完成业务代码的逻辑处理，减少了访问后端`java`服务器的频率，提高访问效率

3. 页面静态化，CDN，静态资源不经过后端服务器

4. 异步化，通过RocketMQ中间件实现。

##### 如何削峰？

在应用本地实现了一个线程池，在应用本地开一个20个线程的线程池，使用submit方法，实现Callable，将下单入库操作放在call方法中。相当于用一个拥塞窗口为20的等待队列，实现泄洪。可用性高，但可能存在负载均衡

可以将队列设置到应用外的Redis内，当redis不可用时，再使用本地的队列

##### 如何防止库存超卖？

1. 活动发布同步库存进Redis缓存
2. 下单交易减Redis缓存库存
3. 异步消息扣减数据库内库存

##### 如何保证系统高可用？

秒杀活动开始时，要限流，可以用秒杀令牌？用队列削峰



##### 如何保证redis和数据库数据一致性？

redis设置失效时间；修改DB后删除redis中的缓存；

写入的时候直接更改DB，读的时候读Cache