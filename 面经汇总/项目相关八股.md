#### Redis

##### Redis基本数据类型

key是字符串，value是5种：String、List、Hash、Set、Zset

**String**：是SDS(Simple Dynamic String)，动态字符串，可修改，最长512M。

>  扩展：为什么不用c语言的字符串？Redis如何解决
>
> * 多增加了字符串长度属性len：c语音中的字符串末尾有'\0'，并且获取长度是O(N)级别的。SDS只需O(1)
>
> * 自动扩展空间：由于字符串经常会进行拼接操作，所以没有提取获取长度，可能会造成缓存区溢出。当 SDS 需要对字符串进行修改时，首先借助于 `len` 和 `alloc` 检查空间是否满足修改所需的要求，如果空间不够的话，SDS 会自动扩展空间，避免了像 C 字符串操作中的覆盖情况；
>
> * 有效降低内存分配次数：SDS优化了修改字符串带来的内存重分配的次数，采用**空间预分配(会分配多余的free空间)和惰性空间释放(缩减字符串后不会马上释放空间)**
>
> * 二进制安全：因为存了长度，不用去判断空字符`\0`



**List**：相当于Java的LinkedList，插入删除O(1)，索引定位O(n)

​	lrange：输出list指定范围的元素`lrange mylist 0 -1`

**Hash**：数组+链表 解决哈希冲突，实际上字典结构内部包含两个HashTable，用于扩容时的**渐进式搬迁**。当hash表中的**元素个数等于第一位数组的长度**时，扩容为原数组的两倍。如果Redis正在做 `bgsave(持久化命令)`，为了减少内存也得过多分离，Redis 尽量不去扩容，但是如果 hash 表非常满了，**达到了第一维数组长度的 5 倍了**，这个时候就会 **强制扩容**。

当 hash 表因为元素逐渐被删除变得越来越稀疏时，Redis 会对 hash 表进行缩容来减少 hash 表的第一维数组空间占用。所用的条件是 **元素个数低于数组长度的 10%**，缩容不会考虑 Redis 是否在做 `bgsave`。

> **渐进式rehash**：大字典扩容需要申请新数组，并将原数组所有元素重新挂接到新数组中，由于单线程，所以采用渐进式rehash，这会在rehash的过程中，保留新旧两个hash结构，查询时同时查两个，当全部迁移完成后，使用新的hash结构取而代之

**Set**：无序、唯一。

##### Redis的zset的底层数据结构？

zset：唯一性，并为每个value赋予一个score值，代表排序权重

跳跃表：有序列表zset的数据结构，链表按照score排序，链表分层，上层的链表树约为下层的1/2，查询的时候从上层开始查，查询效率O(logN)

> 为什么不用红黑树/平衡树？性能和实现考虑

插入节点，根据随机算法来分配合理的层数，从期望上来看，50%的概率被分到第一层，25%的概率被分到第二层，1/4第三层.............

跳表默认最大的层数为32层

插入的大致流程：声明存储变量、搜索当前节点插入的位置、生成插入节点、重排前向指针、重排后向指针并返回

<img src="https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQ5qUqf8c0vC3bfbc710Tz6iadcOlDYb39pApOUP9pCaUDQtuicUn9Jibvg/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

<img src="https://mmbiz.qpic.cn/mmbiz_png/ia1kbU3RS1H5ZLiaicqeR9mzkQuQLwvtFfQclc3x7f7KuQtUMpfn1rP3I7mGVuOyydQjyhujAgTzo9z8XiacD0oJmA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

元素排名：跳跃表在前向指针上增加了一个`span`属性，**用来表示从前一个节点沿着当前层的forward指针跳到当前节点中间跳过了多少节点**。因此可以沿着搜索路径，将所有经过的节点的跨度`span`值加起来就能得到最终的`rank`值

扩展：

Bitmap：布隆过滤器

HyperLogLog：不精确的去重计数功能，适合大规模去重统计

Geospatial：保存地理位置

pub/sub：订阅功能，简单的消息队列

Pipeline：批量执行一组指令，一次性返回结果，可以减少频繁的请求应答

Lua：支持Lua脚本执行一系列的功能。具有原子性

事务：只保证串行执行命令，并保证全部执行，但执行命令失败不会回滚，而是继续执行

##### Redis 和 Memcached 的区别和共同点

MC：多线程异步IO，只支持K-V，最大失效时间30天，key和value大小有限制

Redis：单线程，采用非阻塞的异步事件处理机制，避免线程上下文切换的代价；支持持久化；支持**多种数据结构**；主从同步机制、集群

##### Redis如何支持高并发

主从架构 + 读写分离

一主多从，主负责写，并负责将数据同步到其他slave节点上，从负责读

##### 为什么Redis用单线程而不是多线程

https://draveness.me/whys-the-design-redis-single-thread

CPU不是Redis的瓶颈，Redis的瓶颈是内存大小和网络带宽，数据都在内存中

单线程带来更高的可维护性，方便开发

单线程也可以并发处理客户端请求，IO多路复用

> 4.0后引入多线程？只是在部分命令上引入(多个非阻塞的删除操作)，在整体架构上还是单线程模型

##### Redis的事务是怎么实现的

redis事务是通过multi,exec,discard,watch/unwatch指令用来操作事务。

- mutil：开启事务，此后所有的操作将会添加到当前链接的事务“操作队列”中。

- exec：提交事务

- discard：取消事务，记住，此指令不是严格意义上的“事务回滚”，只是表达了“事务操作被取消”的语义，将会导致事务的操作队列中的操作不会被执行，且事务关闭。

- watch/unwatch：“观察”，被watch的key如果被其他客户端修改，会discard；事务执行成功或discard会导致被watch的key变为unwatch

  > 这里的watch其实实现了一个CAS乐观锁，只有被watch的key没有改变时，才会exec执行事务

原理：EXEC指令将会触发事务中所有的操作被**写入AOF文件**（如果开启了AOF），然后开始在内存中实施这些数据变更操作
如果在EXEC指令被**提交之前**，Redis-server即检测到提交的某个指令存在**语法错误**，那么此事务将会被提前标记**DISCARD**，此后事务提交也将**直接被驳回**；但是如果在**EXEC提交后**，在实施数据变更时（Redis将不会预检测数据类型，比如你对一个“非数字”类型的key执行INCR操作），某个操作导致了**ERROR**，那么redis仍然**不会回滚**此前**已经执行成功的操作**，而且也不会中断ERROR之后的其他操作**继续执行**。

对于开发者而言，你务必关注事务执行后返回的结果（结果将是一个集合，按照操作提交的顺序排列，对于执行失败的操作，结果将是一个ERROR）。

##### 缓存的更新方式

可以在更新完DB后直接更新缓存；设置失效时间，可以看做数据不一致的最大容忍时间，可以在key失效时请求数据源获取新数据，重置失效时间；失效时，异步更新，防止数据源更新时出错

##### 缓存一致性问题

1. 先删除缓存，后更新数据库

   如果删除缓存后，更新数据库的过程中，有线程进行读，缓存被删了，读数据库，读到旧数据，写入缓存，不一致

   解决：延时双删：更新数据库的线程在更新完后，sleep一段时间，然后再删除一次缓存

2. 先更新数据库，再删除缓存

   如果更新成功，缓存删除失败，不一致

   解决：通过消息队列，利用消息队列的重试机制，保证消息的**最终一致性**

3. 其他：设置缓存过期时间。

##### 如果在更新Redis时服务器宕机，怎么办？

持久化、主从、

##### Redis的部署架构是什么样的？

单机、主从、集群

##### Redis内存淘汰机制、几种淘汰策略

设置有效期

删除过期键的策略：定时删除、惰性删除、定时扫描

- **定时删除** ：为每个键设置一个定时器，一旦过期时间到了，则将键删除。这种策略对内存很友好，但是对 `CPU` 不友好，因为每个定时器都会占用一定的 `CPU` 资源。
- **惰性删除** ：不管键有没有过期都不主动删除，等到每次去获取键时再判断是否过期，如果过期就删除该键，否则返回键对应的值。这种策略对内存不够友好，可能会浪费很多内存。
- **定期扫描** ：系统每隔一段时间就定期扫描一次，发现过期的键就进行删除。这种策略相对来说是上面两种策略的折中方案，需要注意的是这个定期的频率要结合实际情况掌控好，使用这种方案有一个缺陷就是可能会出现已经过期的键也被返回。

淘汰没过期键的策略

| 淘汰策略        | 说明                                                         |
| :-------------- | :----------------------------------------------------------- |
| volatile-lru    | 根据 LRU 算法删除设置了过期时间的键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| allkeys-lru     | 根据 LRU 算法删除所有的键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| volatile-lfu    | 根据 LFU 算法删除设置了过期时间的键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| allkeys-lfu     | 根据 LFU 算法删除所有的键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| volatile-random | 随机删除设置了过期时间的键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| allkeys-random  | 随机删除所有键，直到腾出可用空间。如果没有可删除的键对象，且内存还是不够用时，则报错 |
| volatile-ttl    | 根据键值对象的 ttl 属性， 删除最近将要过期数据。如果没有，则直接报错 |
| noeviction      | 默认策略，不作任何处理，直接报错                             |

> 衍生：Redis对LRU算法的改进，解决传统LRU的什么缺点？怎么改进的？抽样删除

Redis热度数据管理：LRU：lru属性24位、全局属性lru_clock；LFU：lru属性高16位时钟低8位频数、频次递增(随机数、概率p公式、对数因子)、频次递减(`lfu-decay-time`、计算差值、除以`lfu-decay-time`)

##### Redis缓存穿透和缓存雪崩？

**缓存雪崩**：Redis中的缓存大面积失效，所有的流量直接打在数据库上，数据库亚历山大啊

解决方案：在往Redis中存数据的时候，设置随机的失效时间；或者考虑热点数据永不失效，有更新操作的时候更新一下缓存

**缓存穿透**：缓存和数据库都没有的数据，而某个不良用户不断地发起请求，比如数据库中的id都大于0，我一直用小于0的id去请求，每次都能避开Redis，直接打在数据库上，造成数据库压力过大

解决方案1：在接口层增加校验，如用户权限校验、参数校验、不合法的直接返回Null给前端。比如id校验id<=0直接拦截。

解决方案2：缓存和数据库都取不到的key，可以将对应key的value设置为null、错误提示等，具体看场景，设置个失效时间30秒。

解决方案3：**布隆过滤器**

**缓存击穿**：是指某个热点key，不停地扛着大并发，当这个key失效的瞬间，持续的大并发就会击穿缓存，直接请求数据库。

解决方案：设置热点数据永不过期，数据有更新时，同时更新缓存即可

**总结**：上述三个问题，之前：Redis高可用，主从+哨兵，Redis集群，避免全盘崩溃

若还是发生了上述问题，采用本地缓存、限流、降级等方法，避免MySQL挂掉

发生问题之后，Redis持久化RDB+AOF，一旦重启，自动从磁盘加载数据，快速恢复缓存数据

哪怕限流，也不能让数据库挂掉，对用户来说多刷几次而已~

##### 布隆过滤器

概念：用于检索一个元素是否在一个集合中。

原理：当元素被加入集合时，通过k个散列函数将这个元素映射成一个位(bit)数组中的k个点，把他们置成1。检索时，只要看对应的这些点是否为1，就大概率知道该集合中是否有该元素。

缺点：有可能误判，可能元素不在集合中，但k个bit位都为1；删除困难

##### Redis持久化的方式

两种持久化方式：RDB和AOF

###### **RDB**

快照是一次全量备份，快照作为包含整个数据集的单个.rdb文件生成，快照是内存数据的二进制序列化形式，在存储上非常紧凑。

使用系统多进程`COW(Copy On Write)`机制的`fork`函数，产生一个**子进程**，**子进程**会拷贝**父进程**的部分**代码段和数据段**，**快照持久**化可以完全由**子进程**完成，**父进程**继续**处理客户端请求**。数据段由操作系统的**页面**组合而成，主进程在对某个页面进行修改时，会得到该页面的一份复制，然后**在复制页上进行修改**，而**子进程**相应的**页面是没有变化**的，子进程只需要遍历数据进行序列化写磁盘就行了。

**触发机制**：手动、自动

自动触发：

1. 在配置文件中设置触发条件

```properties
# 当在规定的时间内，Redis发生了写操作的个数满足条件，会触发发生BGSAVE命令。
# save <seconds> <changes>
# 当用户设置了多个save的选项配置，只要其中任一条满足，Redis都会触发一次BGSAVE操作
save 900 1 
save 300 10 
save 60 10000
# 以上配置的含义：900秒之内至少一次写操作、300秒之内至少发生10次写操作、
# 60秒之内发生至少10000次写操作，只要满足任一条件，均会触发bgsave
```

2. 执行shutdown命令关闭服务器时，如果没有开启AOF，则会自动执行一次`bgsave`

3. 主从同步：master执行`bgsave`，并缓存该时间段的写操作，将rdb发送给slave，slave同步数据，最后master向所有slave发送缓存中的写操作，完成同步

**执行流程**：使用操作系统多进程的COW机制

1. 执行`bgsave`时，先看有不有子进程正在执行RDB/AOF持久化任务，有则返回
2. 主进程`fork`一个子进程来执行RDF，`fork`操作会对主进程造成阻塞，只是`fork`期间阻塞，之后就不阻塞了
3. 子进程根据主进程的内存生成临时快照文件，持久化完成后用临时快照文件替换原来的RDF文件。此过程主进程仍然可以响应写操作，但是是在内存页面的副本进行，不会影响子进程持久化工作
4. 子进程完成后发消息给主进程

**RDB的优缺点**

优点

* RDB文件小，适合定时备份，用于容灾
* Redis加载RDB文件速度比AOF日志快很多，因为RDB存的是内存中的数据，而AOF日志记录的是指令，需要顺序执行所有指令来恢复

缺点

* 无法实时持久化，两次`bgsave`过程中的数据可能丢失
* `fork`子进程会阻塞Redis主进程
* 老版本的Redis可能不兼容新版本RDB格式文件

###### **AOF**

Append Only File，仅追加文件，AOF 日志是连续的增量备份。每次执行**修改内存**中数据集的写操作时，都会**记录**该操作。假设AOF记录了自Redis实例创建以来**所有的修改性指令**，那么就可以通过对一个空的Redis实例**顺序执行所有指令**，即**重放**，来恢复Redis当前实例的内存数据结构的状态

AOF会在持续运行中持续增大，因此需要定期进行AOF重写，对AOF日志进行瘦身

开启方式

```properties
## 此选项为aof功能的开关，默认为“no”，可以通过“yes”来开启aof功能  
## 只有在“yes”下，aof重写/文件同步等特性才会生效  
appendonly yes  

## 指定aof文件名称  
appendfilename appendonly.aof  

## 指定aof操作中文件同步策略，有三个合法值：always everysec no,默认为everysec  
appendfsync everysec  
## 在aof-rewrite期间，appendfsync是否暂缓文件同步，"no"表示“不暂缓”，“yes”表示“暂缓”，默认为“no”  
no-appendfsync-on-rewrite no  

## aof文件rewrite触发的最小文件尺寸(mb,gb),只有大于此aof文件大于此尺寸是才会触发rewrite，默认“64mb”，建议“512mb”  
auto-aof-rewrite-min-size 64mb  

## 相对于“上一次”rewrite，本次rewrite触发时aof文件应该增长的百分比  
## 每一次rewrite之后，redis都会记录下此时“新aof”文件的大小(例如A)
## aof文件增长到A*(1 + p)之后，触发下一次rewrite，每一次aof记录的添加，都会检测当前aof文件的尺寸。  
auto-aof-rewrite-percentage 100
```

Linux对文件采用延迟写入，即每次写入先进缓存，到了已定时机再写入磁盘。

Linux提供了`fsync(int fd)`函数，可以将指定文件的内容强制从内核缓存刷到磁盘，但是是一个磁盘IO操作，非常耗时。因此**Redis提供了3种AOF同步策略**：

* always：每条AOF都立即同步，性能低，安全
* everysec：每秒同步，默认的方式，性能安全较平衡
* no：永不直接同步，全由操作系统决定。性能好但非常不安全

**重写(Rewrite)机制**

不是基于源AOF文件，而是基于当前内存数据，类似于RDB快照的方式，使用更少的指令来记录内存中数据的状态。先存当前内存状态，再将重写期间的写操作从缓存写入AOF。

**触发机制**：手动和自动

手动：`bgrewriteaof`命令：`redis-cli -h ip -p port bgrewriteaof`

自动

```properties
auto-aof-rewrite-min-size:表示运行AOF重写时文件最小体积，默认为64MB（我们线上是512MB）。

auto-aof-rewrite-percentage:代表当前AOF文件空间（aof_current_size）和上一次重写后AOF文件空间（aof_base_size）
```

**AOF的优缺点**

优点：只是追加写日志，写增量信息，对服务器性能影响小，速度比RDB快，内存消耗少

缺点

* 日志文件太大，需要不断重写瘦身，但和RDB文件比还是很大
* 使用AOF恢复数据时，比RDB慢

###### Redis4.0混合持久化

RDB文件和AOF日志结合，AOF日志记录自RDB持久化开始到当前这段时间发生的增量数据。

大量数据使用RDB，性能高，恢复快；增量数据使用AOF，尽量保证数据不丢失

##### Redis主从复制怎么实现的

作用

* 数据冗余：主从复制实现数据的热备份
* 故障恢复：主节点挂了，从节点可以提供服务，实现快速故障恢复
* 负载均衡：主从复制、读写分离，主提供写，从提供读

实现原理

![image-20210406224617472](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210406224617472.png)

**准备阶段 - > 数据同步阶段 - > 命令传播阶段**

slave第一次连接到master时，master会启动子进程生成RDB快照，并把过程中的写请求存到缓存，RDB文件生成后，将RDB文件发给slave，并把缓存中的写操作也发给slave

之后的数据通过AOF日志同步

##### Redis中的哨兵是干啥的

架构

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210406224745330.png" alt="image-20210406224745330"  />

哨兵节点：哨兵系统由一个或多个哨兵节点组成，不存储数据

数据节点：主、从都是数据节点

功能

- **监控（Monitoring）：** 哨兵会不断地检查主节点和从节点是否运作正常。
- **自动故障转移（Automatic failover）：** 当 **主节点** 不能正常工作时，哨兵会开始 **自动故障转移操作**，它会将失效主节点的其中一个 **从节点升级为新的主节点**，并让其他从节点改为复制新的主节点。
  1. 在失效主服务器属下的从服务器当中， 那些被标记为主观下线、已断线、或者最后一次回复 PING 命令的时间大于五秒钟的从服务器都会被 **淘汰**。
  2. 在失效主服务器属下的从服务器当中， 那些与失效主服务器连接断开的时长超过 down-after 选项指定的时长十倍的从服务器都会被 **淘汰**。
  3. 在 **经历了以上两轮淘汰之后** 剩下来的从服务器中， 我们选出 **复制偏移量（replication offset）最大** 的那个 **从服务器** 作为新的主服务器；如果复制偏移量不可用，或者从服务器的复制偏移量相同，那么 **带有最小运行 ID** 的那个从服务器成为新的主服务器。
- **配置提供者（Configuration provider）：** 客户端在初始化时，通过连接哨兵来获得当前 Redis 服务的主节点地址。
- **通知（Notification）：** 哨兵可以将故障转移的结果发送给客户端。

##### 为什么Redis的负载因子设置比HashMap的负载因子大

Redis的负载因子是键值对的数量/长度，HashMap是已使用的数量/长度

##### Redis的分布式锁？

最低保证分布式锁的有效性及安全性的要求如下：

1.互斥；任何时刻只能有一个client获取锁

2.释放死锁；即使锁定资源的服务崩溃或者分区，仍然能释放锁

3.容错性；只要多数redis节点（一半以上）在使用，client就可以获取和释放锁

**问题**

因为redis在进行主从复制时是异步完成的，比如在clientA获取锁后，主redis复制数据到从redis过程中崩溃了，导致没有复制到从redis中，然后从redis选举出一个升级为主redis,造成新的主redis没有clientA 设置的锁，这是clientB尝试获取锁，并且能够成功获取锁，导致互斥失效；

**Redis分布式锁的实现**

**单实例**中的实现：`SET key_name value_name NX PX 30000`保证原子性

> NX 表示if not exist 就设置并返回True，否则不设置并返回False   PX 表示过期时间用毫秒级， 30000 表示这些毫秒时间后此key过期

获取失败则等待一段时间重试(大于ttl)或退出

获取锁，完成相关操作后，必须删除自己的锁。这里获取和删除都是用lua脚本，保证操作的原子性

**多节点**Redis实现(RedLock)：有效防止单点故障

1.获取当前时间戳

2.client尝试按照顺序使用相同的key,value获取所有redis服务的锁，在获取锁的过程中的获取时间比锁过期时间短很多，这是为了不要过长时间等待已经关闭的redis服务。并且试着获取下一个redis实例。

  比如：TTL为5s,设置获取锁最多用1s，所以如果一秒内无法获取锁，就放弃获取这个锁，从而尝试获取下个锁

3.client通过获取所有能获取的锁后的时间减去第一步的时间，这个时间差要小于TTL时间并且至少有3个redis实例成功获取锁，才算真正的获取锁成功

4.如果成功获取锁，则锁的真正有效时间是 TTL减去第三步的时间差 的时间；比如：TTL 是5s,获取所有锁用了2s,则真正锁有效时间为3s(其实应该再减去时钟漂移);

5.如果客户端由于某些原因获取锁失败，便会开始解锁所有redis实例；因为可能已经获取了小于3个锁，必须释放，否则影响其他client获取锁



##### Redis包含的模块

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210412161400785.png" alt="image-20210412161400785" style="zoom:67%;" />

##### Redis为什么快？

1. 纯内存操作
2. 单线程，没有各自乱七八糟的锁
3. 多路IO复用模型，底层有很多优化
4. 高效的数据结构：如不同长度的字符串用了不同的结构体、HyperLogLog的密集型存储结构等

##### Redis的HyperLogLog

HyperLogLog是一种估计基数的近似最优算法。基数统计：用来统计一个集合中不重复的元素个数，比如统计网站每个页面的UV(独立访客)

大致的原理：一个随机数，尾部有1个0的概率为1/2，两个0的概率为1/4，...，尾部k个0的概率为2^(-k)^。因此我们可以通过尾部连续0的最大数量k，估算出我们用了多少个随机数。HyperLogLog通过分配若干桶(2^14^=26384)，对每个桶得到的最大数量k进行调和平均，得到一个k#，k#为一个浮点数。具体还有很多修正因子，比较复杂

占用内存仅12KB，2^14^个桶，每个6bit，2^14^ *6/8

Redis的实现：计数量小的时候，转换成稀疏存储方式；否则采用密集存储。最大限度节约内存

密集存储：16384个6bit连续成串，8bit 1字节，需要一些移位拼接的处理

稀疏存储：当某个计数值需要调整到大于32时，会转换为密集存储

* 00开头表示后6位整数值加1就是零值计数器的数量
* 01后14bit最多可以表示连续16384个零值计数器
* 1vvvvvxx：中间5bit计数，后2bit表示连续几个桶

指令：`pfadd`和`pfcount`

https://mp.weixin.qq.com/s/9dtGe3d_mbbxW5FpVPDNow

https://cloud.tencent.com/developer/article/1349691

##### 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？

使用 `keys` 指令可以扫出指定模式的 key 列表。但是要注意 keys 指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用 `scan` 指令，`scan` 指令可以无阻塞的提取出指定模式的 `key` 列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用 `keys` 指令长。





#### `SpringBoot`

##### `SpringBoot`如何自动装配

* `@SpringBootApplication`这个注解的作用

  主要是三个注解

  * `@SpringBootConfiguration`(实质就是一个`@Configuration`)：代表当前类是一个配置类
  * @ComponentScan：根据指定的配置，扫描`package`

  * **`@EnableAutoConfiguration`**

    ```java
    @AutoConfigurationPackage
    @Import(AutoConfigurationImportSelector.class)
    public @interface EnableAutoConfiguration {}
    ```

    1. @AutoConfigurationPackage

       自动配置包，指定了默认的包规则

       ```java
       @Import(AutoConfigurationPackages.Registrar.class)  //给容器中导入一个组件
       public @interface AutoConfigurationPackage {}
       
       //利用Registrar给容器中导入一系列组件
       //将指定的一个包下的所有组件导入进来？MainApplication 所在包下。
       ```

    2. @Import(AutoConfigurationImportSelector.class)

       ```java
       1、利用selectImports方法里的getAutoConfigurationEntry(annotationMetadata);给容器中批量导入一些组件
       2、getAutoConfigurationEntry方法中，调用List<String> configurations = getCandidateConfigurations(annotationMetadata, attributes)获取到所有需要导入到容器中的候选配置类，127个组件
       3、利用工厂加载 Map<String, List<String>> loadSpringFactories(@Nullable ClassLoader classLoader)；获取资源文件，位置是META-INF，得到所有的组件
       4、从META-INF/spring.factories位置来加载一个文件。
           默认扫描我们当前系统里面所有META-INF/spring.factories位置的文件
           spring-boot-autoconfigure-2.3.4.RELEASE.jar包里面也有META-INF/spring.factories
       ```

    ![image.png](https://cdn.nlark.com/yuque/0/2020/png/1354552/1602845382065-5c41abf5-ee10-4c93-89e4-2a9b831c3ceb.png)

    文件里面写死了spring-boot一启动就要给容器中加载的所有配置类

    虽然我们127个场景的所有自动配置启动的时候默认全部加载。xxxxAutoConfiguration
    按照条件装配规则（@Conditional），最终会按需配置。

    

##### `Spring`注入依赖的方式、原理

1. 属性注入：通过set方法注入Bean的属性或依赖对象，比较灵活

   ```xml
   <bean id=".." class="com.xxx.xx.x">
   	<property name="aaa" value="bbb"/>
   </bean>
   ```

2. 构造函数注入

   1. 按类型匹配入参

      ```xml
      <!--1.根据参数类型注入-->
      <bean id="car1" class="com.vtstar.entity.Car">
          <constructor-arg type="int" value="300"/>
          <constructor-arg type="java.lang.String" value="红旗"/>
          <constructor-arg type="double" value="20000000.9"/>
      </bean>
      ```

   2. 按索引匹配入参：如果构造函数有相同类型的入参，第一种用不了

      ```xml
      <!--2.通过入参位置下标-->
      <bean id="car2" class="com.vtstar.entity.Car">
          <constructor-arg index="0" value="400"/>
          <constructor-arg index="1" value="大众辉腾"/>
          <constructor-arg index="2" value="20000000"/>
      </bean>
      ```

   3. 联合使用类型和索引匹配入参

      ```java
          public Car(String brand, String corp, double price) {
              System.out.println("brand :" + brand + " corp :" + corp + " price :"+price);
              this.brand = brand;
              this.corp = corp;
              this.price = price;
          }
          public Car(String brand, String corp, int maxSpeed) {
              System.out.println("brand :" + brand + " corp :" + corp + " maxSpeed :"+maxSpeed);
              this.brand = brand;
              this.corp = corp;
              this.maxSpeed = maxSpeed;
          }
      ```

      ```xml
      <!--3.通过参数类型和入参位置联合注入-->
      <bean id="car3" class="com.vtstar.entity.Car">
          <constructor-arg index="0" type="java.lang.String" value="30000000.0"/>
          <constructor-arg index="1" type="java.lang.String" value="卡迪拉克"/>
          <constructor-arg index="2" type="int" value="400"/>
      </bean>
      ```

      第三个属性位置相同但类型不同，需结合下标和类型

   4. 自身类型反射入参

      ```xml
      <!--4.通过自身类型反射入参-->
      <bean id="boss" class="com.vtstar.entity.Boss">
          <constructor-arg value="Tom"/>
          <constructor-arg ref="car1"/>
          <constructor-arg value="20"/>
      </bean>
      ```

      

   构造函数注入方式优点：

   1. 可以保证一些重要的属性在Bean实例化的时候就设置好，
   2. 不用setter方法
   3. 更好地封装

   缺点：

   1. 类属性太多，可读性降低
   2. 不灵活
   3. 多个构造函数复杂
   4. 不利于类的继承和拓展
   5. 可能会循环依赖

3. 工厂注入

   分为静态工厂和非静态工厂

   ```java
   public class CarFactory {
       
       public Car createHongQiCar(){
           Car car = new Car();
           car.setBrand("红旗H1");
           System.out.println("这里是非静态工厂的创建..." + car.getBrand());
           return car;
       }
   
       public static Car createDaZhongCar(){
           Car car = new Car();
           car.setBrand("大众GoGoGo");
           System.out.println("这里是静态工厂的创建..." + car.getBrand());
           return car;
       }
   }
   ```

   ```xml
   <!--非静态注入工厂方法-->
   <bean id="carFactory" class="com.vtstar.ioc.CarFactory"/>
   <bean id="car5" factory-bean="carFactory" factory-method="createHongQiCar"/>
   
   <!--静态注入工厂方法-->
   <bean id="car6" class="com.vtstar.ioc.CarFactory" factory-method="createDaZhongCar"/>
   ```

   

##### `Spring` `Bean`生命周期、`Spring`生命周期方法

![img](https://pic1.zhimg.com/80/v2-8787f1b3800b71f6234e996d432e0a0c_1440w.jpg?source=1940ef5c)

Spring启动，查找并加载需要被Spring管理的bean，进行Bean的实例化

Bean实例化后对将Bean的引入和值注入到Bean的属性中

如果Bean实现了BeanNameAware接口的话，Spring将Bean的Id传递给setBeanName()方法

如果Bean实现了BeanFactoryAware接口的话，Spring将调用setBeanFactory()方法，将BeanFactory容器实例传入

如果Bean实现了ApplicationContextAware接口的话，Spring将调用Bean的setApplicationContext()方法，将bean所在应用上下文引用传入进来

如果Bean实现了BeanPostProcessor接口，Spring就将调用他们的postProcessBeforeInitialization()方法。

如果Bean 实现了InitializingBean接口，Spring将调用他们的afterPropertiesSet()方法。类似的，如果bean使用init-method声明了初始化方法，该方法也会被调用

如果Bean 实现了BeanPostProcessor接口，Spring就将调用他们的postProcessAfterInitialization()方法。

此时，Bean已经准备就绪，可以被应用程序使用了。他们将一直驻留在应用上下文中，直到应用上下文被销毁。

如果bean实现了DisposableBean接口，Spring将调用它的destory()接口方法，同样，如果bean使用了destory-method 声明销毁方法，该方法也会被调用。

##### 

##### `Spring IOC`原理和缺点

要讲IOC，先解释**依赖倒置**原则。举一个汽车生产的例子。比如米其林生产轮胎，如果我们的汽车根据轮胎生产底盘，根据底盘生产车身，根据车身设计车子，那么如果轮胎发生变化，我们整个车子都要重新设计。他们的依赖关系是车子依赖车身，车身依赖底盘，底盘依赖轮子，即高层依赖低层。这样低层代码一改，上层都要跟着改。。。

~~我们将依赖倒置，即**低层依赖高层**。轮子依赖底盘，底盘依赖车身，车身依赖车子。代码层面将**低层的类**作为**参数**传递给上层。这样若要修改轮胎，就不会影响到上层。~~

**依赖倒置**：高层模块不依赖于低层模块，**两者都应该依赖抽象**，抽象不依赖于细节，细节应该依赖于抽象。这里可以理解为双方不互相依赖，而是依赖第三方。**解耦**

**控制反转(IOC)**就是**依赖倒置原则**的一种代码设计思路。具体采用的方法就是**依赖注入**(DI)。我们可以采用**构造函数**、**Setter**和**接口**的方式传入参数。

依赖注入：甲方开放接口，在需要的时候将乙方传递进来(注入)

控制反转：甲乙双方互不依赖，双方的交易活动互不依赖，而由第三方来管理

由上我们可以得知，初始化一个高层的类时，我们就需要写大量的new，为了解决这个问题，就有了**控制反转容器**(IOC 容器)。这个容器可以可以通过配置文件，自动对代码进行初始化。好处是不用自己从底层往上new了。IOC容器会从高层向下查依赖关系，到达底层后一步步向上new，类似DFS。

Spring的IOC，是将控制权(创建对象和对象之间的依赖关系的权利)交给Spring容器，将耦合代码放到XML文件中统一管理

IOC的优点：解耦，提高可维护性

IOC的缺点：通过反射创建对象，效率有损耗。





##### AOP原理

面向切面编程，将与业务无关，但是为业务模块所调用的逻辑封装起来。业务是核心关注点，其他关系不大的是横切关注点。横切关注点各处基本相似，如权限验证、日志、事务处理。

切面aspect：切点pointcut和advice处理

pointcut切点：在业务代码中的哪里进行增强

advice：做什么增强，在哪里增强。before、after、around、afterReturning、afterThrowing



一个接口想设置多个切面？如何管理执行顺序？通过@Order注解，数字越小，越先执行

相关注解：

**`@pointcut：`**用来定义一个切点

`execution(* com.mutest.controller..*.*(..)))`

第一个 * 号表示返回类型

包名：需要拦截的包名，两个 句点表示当前包和当前包的所有子包

第二个 * 号表示所有的类

*(..)： * 表示所有方法，括号两个点表示任何参数

**`@Around：`**可自由选择执行动作与目标方法的执行顺序，可以改变参数值。需要**线程安全**的情况下使用

**第一个形参必须是 `ProceedingJoinPoint` 类型**，调用`ProceedingJoinPoint`的`proceed`方法才会执行目标方法。还可以传入一个`Object[ ]`对象，该数组中的值将被传入目标方法作为实参——**这就是`Around`增强处理方法可以改变目标方法参数值的关键**

普通使用Before和AfterReturning可以解决，如果目标方法执行前后要共享某种状态数据，则需要使用Around

@Before和@After，目标方法执行后和目标方法执行前，可以做一些统计或者日志

@AfterReturning，可以捕获切入方法的返回值，对返回值进行业务逻辑上的处理

@AfterThrowing：被切方法抛出异常后，进入注解的方法中执行

##### Spring事务隔离级别(和数据库那块差不多)

并发事务引起的问题：

脏读：一个事务读取了另一个事务修改但未提交的数据。如果写操作回滚了，第一个事务读取的数据就无效了

不可重复读：一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间进行了**更新**。

幻读：它发生在一个事务（T1）**读取了几行数据**，接着另一个并发事务（T2）**插入**了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。

**隔离级别：**

ISOLATION_DEFAULT：使用后端数据库默认的隔离级别

ISOLATION_READ_UNCOMMITTED：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读

ISOLATION_READ_COMMITTED： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生

ISOLATION_REPEATABLE_READ：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生

ISOLATION_SERIALIZABLE：最高的隔离级别，完全服从ACID的隔离级别，确保阻止脏读、不可重复读以及幻读，也是最慢的事务隔离级别，因为它通常是通过完全锁定事务相关的数据库表来实现的



##### Spring事务传播机制/传播特性？

七种传播行为

PROPAGATION_REQUIRED：当前方法必须运行在事务中，如果当前事务存在，方法将会在该事务中运行。否则，会启动一个新的事务

PROPAGATION_REQUIRED_NEW：表示当前方法必须运行在它自己的事务中。一个新的事务将被启动。如果外层事务失败回滚了，内层事务的结果仍然会被提交

PROPAGATION_SUPPORTS：是否使用事务取决于调用方法是否有事务，如果有则直接用，如果没有则不使用事务

PROPAGATION_NOT_SUPPORTED：不为当前方法开启事务，该方法运行期间，当前事务被挂起

PROPAGATION_MANDATORY：必须在已有事务下被调用，否则抛出异常 `IllegalTransactionStateException`

PROPAGATION_NEVER：不应该运行在事务上下文中，如果当前有事务抛出异常

PROPAGATION_NESTED：当前已经存在一个事务，那么该方法将会在嵌套事务中运行。嵌套的事务可以独立于当前事务进行单独地提交或回滚。Nested事务回滚，不影响外部事务。只回滚自己内部执行的SQL，不回滚主方法的SQL

##### 工厂模式在Spring源码中的应用

##### Spring中的常用注解

1. @SpringBootApplication：main方法的注解

   三个注解：@SpringBootConfiguration、@EnableAutoConfiguration、@ComponentScan

2. 我们一般使用 `@Autowired` 注解让 Spring 容器帮我们自动装配 bean。要想把类标识成可用于 `@Autowired` 注解自动装配的 bean 的类,可以采用以下注解实现：

   - `@Component` ：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注。
   - `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作。
   - `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。
   - `@Controller` : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。

3. @Repository：对应Dao层，用于数据库相关操作

4. @RestController：@Controller和@ResponseBody的集合，用于前后端分离，返回JSON或XML数据

5. @Configuration：配置类

6. @GetMapping("users")：Get请求，同@RequestMapping(value="/users",method=RequestMethod.GET)

7. @PostMapping("users")：POST请求，同@RequestMapping(value="/users",method=RequestMethod.POST)

8. @RequestBody：读取request请求的body部分

9. @Transactional：该方法开启事务，7个事务传播行为

字段验证注解

1. @NullEmpty：非null非空
2. @NullBlank：非null，必须包含一个非空白字符
3. @NotNull：必须不为null
4. @Min(Value)：必须是数字，且大于等于最小值
5. @Max(Value)：必须是数字，且小于等于最大值
6. Size(max, min)：范围内

##### Spring启动流程



##### Spring IOC容器启动流程(待补全)

从`ClassPathXMLApplicationContext`的构造函数说起，核心方法是**`refresh()`**，`refresh`是`synchronized`加锁的

* `prepareRefresh()`：记录启动时间、标记已启动状态、处理占位符、校验`xml`文件

* `obtainFreshBeanFactory()`：初始化BeanFactory、加载Bean、注册Bean，这步并没有初始化Bean

  * 调用`refreshBeanFactory()`

    判断当前ApplicationContext是否已经加载过BeanFactory，若有则销毁Bean，关闭BeanFactory；否则初始化一个**`DefaultListableBeanFactory`**，然后序列化，再设置BeanFactory**是否允许Bean覆盖**和是否允许循环使用，最后加载Bean到BeanFactory中

    > `ApplicationContext`继承自`BeanFactory`，实际上是其内部持有一个实例化的`BeanFactory`，以后所有的BeanFactory相关操作都由这个实例来做

    为什么用**`DefaultListableBeanFactory`**，它继承自ConfigurableListableBeanFactory和Autowire...那部分，ConfigurableListableBeanFactory继承了`BeanFactory`的所有三个子类，所以**`DefaultListableBeanFactory`**最牛

  * **`BeanDefinition`**接口定义：

    > Bean可以简单的认为是`BeanDefinition`的实例，`BeanDefinition`里面包含了Bean的信息(指向哪个类、是否单例、是否懒加载、依赖了哪些Bean)

    继承父Bean的配置信息、设置Bean的类名称、是否懒加载、Bean的所有依赖、生成该Bean的工程名称(如果有)、属性值、是否单例等

  * customizeBeanFactory：配置是否允许BeanDefinition覆盖(相同id或name)、是否允许循环引用

  * **loadBeanDefinitions(beanFactory) 方法**：加载各个Bean，并放入BeanFactory中

    给这个BeanFactory实例化一个XmlBeanDefinitionReader



##### Spring如何解决循环依赖

1. 什么是循环依赖？

   循环依赖就是循环引用，A依赖B，B依赖C，C依赖A。

   Spring中循环依赖的场景：构造器循环依赖、Field属性的循环依赖

2. 怎么检测循环依赖？

**SpringBean的加载过程**

代码入口！

```java
ApplicationContext ac = new ClassPathXmlApplicationContext("spring.xml");
ac.getBean(XXX.class);
```

ClassPathXmlApplicationContext是一个加载XML配置文件的类

它的构造方法中的核心方法是`refresh()`方法

>  `refresh`方法是加了`synchronized`对象锁的，好处是避免`close()`方法和`refresh()`方法调用冲突，这里对象锁相当于整个方法加锁，同步范围更小，锁的粒度更小，效率更高

下面来分析`refresh()`中的一些核心方法

**obtainFreshBeanFactory方法**

这个方法是获取刷新Spring上下文的Bean工厂

核心是`obtainFreshBeanFactory()`里的`refreshBeanFactory()` 

中的**`this.loadBeanDefinitions(beanFactory)`**

这个`BeanDefinition`可以看成是IOC过程中的一个产物，可以看成是对Bean定义的抽象，里面的数据是与Bean定义相关的，如属性、初始化方法、销毁方法等

这里的主要方法是`loadBeanDefinitions`，主要做了以下几件事：

1. 初始化了`BeanFactoryReader`
2. 通过`BeanFactoryReader`获取`Resource`，也就是配置文件的位置，将`Resource`转成`Document`对象
3. 将`Document`对象转成容器内部的数据结构(`BeanDefinition`)，也就是解析`Bean`定义的各种元素，转换成`Managed`类(`Spring`对`BeanDefinition`的封装)放在`BeanDefinition`中
4. 解析完后，把解析结果放在`BeanDefinition`对象中，并放到一个`Map`中

以上完成了`BeanDefinition`在`IOC`容器中的注册。本质就是`Spring`加载`XML`配置文件或者注解，解析成`BeanDefinition`

**`Spring`创建`Bean`的过程**

回到`refresh()`方法，刚才的`obtainFreshBeanFactory`方法完成了BeanDefinition的注册，即得到了Bean的相关数据，下面就需要获取`Bean`实例。

这里核心方法是`finishBeanFactoryInitialization`里的`preInstantiateSingletons`里的**`getBean()`**方法

`getBean`方法最终调用的是**`doGetBean`**方法，这里就是依赖注入`DI`发生的地方

**Spring创建好了BeanDefinition之后呢，会开始实例化Bean，并且对Bean的依赖属性进行填充。实例化时底层使用了CGLIB或Java反射技术。上图中instantiateBean和PopulateBean方法很重要！**

**循环依赖问题分析**

在**`DefaultListableBeanFactory`**类中，有如下属性：

```java
一级缓存：
/** 保存所有的singletonBean的实例 */
private final Map<String, Object> singletonObjects = new ConcurrentHashMap<String, Object>(64);

二级缓存：
/** 保存所有早期创建的Bean对象，这个Bean还没有完成依赖注入 */
private final Map<String, Object> earlySingletonObjects = new HashMap<String, Object>(16);
三级缓存：
/** singletonBean的生产工厂*/
private final Map<String, ObjectFactory<?>> singletonFactories = new HashMap<String, ObjectFactory<?>>(16);
 
/** 保存所有已经完成初始化的Bean的名字（name） */
private final Set<String> registeredSingletons = new LinkedHashSet<String>(64);
 
/** 标识指定name的Bean对象是否处于创建状态  这个状态非常重要 */
private final Set<String> singletonsCurrentlyInCreation =
	Collections.newSetFromMap(new ConcurrentHashMap<String, Boolean>(16));

allowEarlyReference是否允许从singletonFactories中通过getObject拿到对象
```

简单来说，一级缓存就是单例对象的缓存，二级缓存就是初始化一半的单例对象，三级缓存是单例对象工厂的缓存

若A依赖B，B又依赖A：

![img](https://user-gold-cdn.xitu.io/2018/11/17/16720b0a8ca086c6?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

**本质就是三级缓存发挥作用，解决了循环**

**getSingleton的过程：**首先从`singletonObject`一级缓存中获取，若没有，则尝试从earlySingletonObjects二级缓存中找，如果没有且允许从singletonFactories通过getObject获取，则去三级缓存中获取，如果获取到了，则移除对应的singletFactory，将singletonObject放到earlySingletonObject，也就是将三级缓存升到二级缓存中









如果A中构造注入了B，那么A在关键的方法`addSingletonFactory`之前就去初始化了B，导致三级缓存中根本没有A，发发生死循环，Spring会抛出异常。

#####  







#### RocketMQ

##### 消息队列的原理？

##### 消息队列消息放不下怎么办

##### 消息重试机制

##### 怎么保证消息不丢失

##### 如何保证消息有序性

##### RocketMQ事务原理

##### MQ的选型和各种MQ的区别



#### Nginx

##### 为什么用Nginx？对项目最大的作用是什么





#### MySQL

##### MySQL的基本架构

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210407221000232.png" alt="image-20210407221000232" style="zoom:67%;" />

连接器：负责跟客户端建立链接(TCP)、获取权限(用户名密码)、维持和客户度的连接

> 连接默认的超时时间为8h。可以设置长连接，但是内存消耗很大，可以使用`mysql_reset_connection`重新初始化连接资源，过程不需要重连

查询缓存：拿到一个查询请求，先查缓存，之前是否执行过。如果之前执行过，会用key-value存储在内存中，命中缓存直接返回。但是缓存命中率如果很低，不如不用。更新就会清空缓存。

> **query_cache_type**设置成为DEMAND，不使用缓存

分析器：进行词法分析，识别关键字、表名、列名等；然后进行语法分析，会判断是否有语法错误

优化器：我们有很多索引嘛，优化就是确认用哪个索引更高效；还对执行顺序进行优化，根据条件，先查哪个表，还是先关联。

执行器：执行优化器优化的操作

##### InnoDB的架构

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210407222516528.png" alt="image-20210407222516528" style="zoom:50%;" />

分两大块·

* InnoDB In-Memory Structures
* InnoDB On-Disk Structures

**InnoDB的内存架构**

1. Buffer Pool

   正如之前提到的，MySQL 不会直接去修改磁盘的数据，因为这样做太慢了，MySQL 会先改内存，然后记录 redo log，等有空了再刷磁盘，如果内存里没有数据，就去磁盘 load。

   而这些数据存放的地方，就是 Buffer Pool。

   我们平时开发时，会用 redis 来做缓存，缓解数据库压力，其实 MySQL 自己也做了一层类似缓存的东西。

   MySQL 是以「页」（page）为单位从磁盘读取数据的，Buffer Pool 里的数据也是如此，实际上，Buffer Pool 是`a linked list of pages`，一个以页为元素的链表。

   为什么是链表？因为和缓存一样，它也需要一套淘汰算法来管理数据。

   Buffer Pool 采用基于 LRU（least recently used） 的算法来管理内存

   > 分为两个子链表，新列和旧列，旧的占3/8，新的占5/8，midpoint是两个子列表的交界；当页面被访问，会被移动到缓冲池头部；一直未被使用的会逐渐移到尾部，直至被移除

2. Change Buffer

   上面提到过，如果内存里没有对应「页」的数据，MySQL 就会去把数据从磁盘里 load 出来，如果每次需要的「页」都不同，或者不是相邻的「页」，那么每次 MySQL 都要去 load，这样就很慢了。

   于是如果 MySQL 发现你要修改的页，不在内存里，就把你要对页的修改，先记到一个叫 Change Buffer 的地方，同时记录 redo log，然后再慢慢把数据 load 到内存，load 过来后，再把 Change Buffer 里记录的修改，应用到内存（Buffer Pool）中，这个动作叫做 **merge**；而把内存数据刷到磁盘的动作，叫 **purge**：

   - **merge：Change Buffer -> Buffer Pool**
   - **purge：Buffer Pool -> Disk**

   > Change Buffer 只在操作「二级索引」（secondary index）时才使用，原因是「聚簇索引」（clustered indexes）必须是「唯一」的，也就意味着每次插入、更新，都需要检查是否已经有相同的字段存在，也就没有必要使用 Change Buffer 了；另外，「聚簇索引」操作的随机性比较小，通常在相邻的「页」进行操作，比如使用了自增主键的「聚簇索引」，那么 insert 时就是递增、有序的，不像「二级索引」，访问非常随机。

3. Adaptive Hash Index

   MySQL 索引，不管是在磁盘里，还是被 load 到内存后，都是 B+ 树，B+ 树的查找次数取决于树的深度。你看，数据都已经放到内存了，还不能“一下子”就找到它，还要“几下子”，这空间牺牲的是不是不太值得？

   尤其是那些频繁被访问的数据，每次过来都要走 B+ 树来查询，这时就会想到，我用一个指针把数据的位置记录下来不就好了？

   这就是「自适应哈希索引」（Adaptive Hash Index）。自适应，顾名思义，MySQL 会自动评估使用自适应索引是否值得，如果观察到建立哈希索引可以提升速度，则建立。

4. Log Buffer

   Log Buffer 里的 redo log，会被刷到磁盘里

**Operating System Cache**

内存与磁盘之间的一个高速缓存

**InnoDB的磁盘架构**

除了表结构定义和索引，还有一些为了高性能和高可靠而设计的角色，比如 redo log、undo log、Change Buffer，以及 Doublewrite Buffer 等等.

1. 表空间

   从架构图可以看到，Tablespaces 分为五种：

   - The System Tablespace
   - File-Per-Table Tablespaces
   - General Tablespace
   - Undo Tablespaces
   - Temporary Tablespaces

   其中，我们平时创建的表的数据，可以存放到 The System Tablespace 、File-Per-Table Tablespaces、General Tablespace 三者中的任意一个地方，具体取决于你的配置和创建表时的 sql 语句。

2. Doublewrite Buffer

   **如果说 Change Buffer 是提升性能，那么 Doublewrite Buffer 就是保证数据页的可靠性。**

   前面提到过，MySQL 以「页」为读取和写入单位，一个「页」里面有多行数据，写入数据时，MySQL 会先写内存中的页，然后再刷新到磁盘中的页。

   这时问题来了，假设在某一次从内存刷新到磁盘的过程中，一个「页」刷了一半，突然操作系统或者 MySQL 进程奔溃了，这时候，内存里的页数据被清除了，而磁盘里的页数据，刷了一半，处于一个中间状态，不尴不尬，可以说是一个「不完整」，甚至是「坏掉的」的页。

   有同学说，不是有 Redo Log 么？其实这个时候 Redo Log 也已经无力回天，Redo Log 是要在磁盘中的页数据是正常的、没有损坏的情况下，才能把磁盘里页数据 load 到内存，然后应用 Redo Log。而如果磁盘中的页数据已经损坏，是无法应用 Redo Log 的。

   所以，MySQL 在刷数据到磁盘之前，要先把数据写到另外一个地方，也就是 Doublewrite Buffer，写完后，再开始写磁盘。Doublewrite Buffer 可以理解为是一个备份（recovery），万一真的发生 crash，就可以利用 Doublewrite Buffer 来修复磁盘里的数据。

(待补全。。。

##### InnoDB和MyISAM区别，优缺点

- 事务：InnoDB 是事务型的，可以使用 Commit 和 Rollback 语句。
- 并发：MyISAM 只支持表级锁，而 InnoDB 还支持行级锁。
- 外键：InnoDB 支持外键。
- 备份：InnoDB 支持在线热备份。
- 崩溃恢复：MyISAM 崩溃后发生损坏的概率比 InnoDB 高很多，而且恢复的速度也更慢。
- 其它特性：MyISAM 支持压缩表和空间数据索引。



##### 一条SQL语句执行过程

查询语句：先检查该语句是否有权限，若有权限，则先查询缓存，即以这条SQL为key内存中是否有查询结果，若没有，则通过分析器进行词法分析，提取关键元素，如select，提取表名、需查询的列，判断SQL语句是否有语法错误，然后优化器根据优化算法，执行效率最好的方案。

更新语句：先查询数据，查缓存，修改更新的数据，调用引擎的API接口，写入这行数据，InnoDB把数据保存在内存中，同时记录redo log，此时redo log进入prepare状态，然后告诉执行器，执行完成了，随时可以提交。执行器收到通知后记录binlog，然后调用引擎接口，提交redo log为提交状态，更新完成

##### 事务隔离级别

Read committed、Read Uncommitted、Repeatable Read、Serializable

##### 可重复读会产生幻读吗？

会，可重复读只能解决脏读和不可重复读

##### InnoDB怎么解决幻读

**快照读**就是简单的 `select * from table where ....`

**当前读**就是 

`select * from table where ? lock in share mode` -- 共享锁 （这里可能不同版本的mysql对应命令不一样）

`select * from table where ? for update` -- 排他锁

`insert into table values(?)`

`update table set ? where ?`

`delete from table where ?`

**当前读**：在RR级别下，使用next-key locks来锁住本条记录以及索引区间

**普通(快照)读**：MVCC



**B+Tree原理**

B树是一颗平衡查找树，所有叶子在同一层

B+树是基于**B树**和**叶子节点顺序访问指针**进行实现的

B+树的两种节点：内部节点(索引节点)和叶节点，内部节点不存数据，只存索引；数据都在叶子结点内

内部节点中的key按顺序排列，对于内部节点的一个key，左子树都**小于**它，右子树都**大于等于**它，因此叶节点也是有序的，所有叶子都存有相邻节点的指针

查找类似于二叉查找树，节点内用二分

插入，寻找插入位置，判断是否需要拆分节点

B+树和红黑树相比，B+树的高度更低，磁盘IO次数更少

B+树和B树相比，由于内部节点只存索引，因此内部节点更小，如果一次性把所有同一内部节点的key存到一块磁盘，那么读取的效率变高，IO次数降低；B+树查询路径相同，都是根到叶，效率更稳定；**遍历效率高**



MySQL的索引

索引是帮助MySQL高效获取数据的**数据结构**

B+树索引

- 因为不再需要进行全表扫描，只需要对树进行搜索即可，所以查找速度快很多。
- 因为 B+ Tree 的有序性，所以除了用于查找，还可以用于排序和分组。
- 可以指定多个列作为索引列，多个索引列共同组成键。
- 适用于全键值、键值范围和键前缀查找，其中键前缀查找只适用于最左前缀查找。如果不是按照索引列的顺序进行查找，则无法使用索引。

哈希索引

O(1)查找，但无序

InnoDB有一个叫“自适应哈希索引”，当某个索引值被使用非常频繁，会在B+Tree索引之上再创建一个哈希索引

全文索引

用于查找文本中的关键词

空间数据索引

MyISAM支持，用于地理数据存储，会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。

##### 聚集索引和非聚集索引

聚簇索引：指主索引的叶子节点data域记录着完整的数据记录，一个表只有一个聚簇索引

1. 在表上定义主键PRIMARY KEY，InnoDB将主键索引用作聚簇索引。
2. 如果表没有定义主键，InnoDB会选择第一个不为NULL的唯一索引列用作聚簇索引。
3. 如果以上两个都没有，InnoDB 会使用一个6 字节长整型的隐式字段 ROWID字段构建聚簇索引。该ROWID字段会在插入新行时自动递增。

辅助索引的叶子结点只记录主键的值，如果用辅助索引查找，需要先找到主键，再去主索引中查找，这个过程叫回表

##### 聚集索引为啥查询快

对数据进行聚集，那么只需读取少数的数据页就能读到全部数据，减少了IO次数；与普通索引相比，减少一次回表

##### 组合索引

![image-20210408161825300](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210408161825300.png)

##### 最左匹配原则

最左匹配原则和联合索引的索引存储结构和检索方式有关

在组合索引树中，最底层的叶子节点按照第一列a列从左到右递增排列，但是b列和c列是无序的，b列只有在a列值相等的情况下小范围内递增有序，而c列只能在a，b两列相等的情况下小范围内递增有序。

就像上面的查询，B+树会先比较a列来确定下一步应该搜索的方向，往左还是往右。如果a列相同再比较b列。但是如果查询条件没有a列，B+树就不知道第一步应该从哪个节点查起。

可以说创建的idx_abc(a,b,c)索引，相当于创建了(a)、（a,b）（a,b,c）三个索引。、

**组合索引的最左前缀匹配原则：使用组合索引查询时，mysql会一直向右匹配直至遇到范围查询(>、<、between、like)就停止匹配。**

where a = 1 and b > 10 and c = 10怎么走

a和b会用到索引

##### SQL语句命中多个索引组合时如何选择使用哪组索引

##### 索引的几大原则

1. 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。

2. =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。

3. **尽量选择区分度高的列作为索引**，区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，那可能有人会问，这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。

4. 索引列不能参与计算，保持列“干净”，比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。

5. 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。



**覆盖索引**：指索引的叶节点已经包含所有需要查询的字段的值。这样就不需要回表查询主索引了，提高效率；并且只读取索引，大大减少数据访问量

##### 索引的优点

* 大大减少服务器需要扫描的数据行数

* 帮助服务器避免进行排序和分组，避免创建临时表(主要是排序和分组时创建的，有了B+Tree就不需要排序和分组了)

* 将随机IO变为顺序IO，相邻数据存在一起，提高读取效率

##### 索引的使用条件

- 对于非常小的表、大部分情况下简单的全表扫描比建立索引更高效；
- 对于中到大型的表，索引就非常有效；
- 但是对于特大型的表，建立和维护索引的代价将会随之增长。这种情况下，需要用到一种技术可以直接区分出需要查询的一组数据，而不是一条记录一条记录地匹配，例如可以使用分区技术（应该就是垂直拆分和水平拆分？）。



##### InnoDB一棵B+树可以存放多少行数据？

约2千万或更多(增加高度为4-5层，一般为3层)...不确定



##### MySQL5.6对索引做了什么优化？

MySQL 5.6引入了索引**下推优化(ICP)**

```sql
select * from T where name like '陆%' and age = 100；
```

* 没有索引下推优化时：

对于联合索引index(name,age)，我们知道，根据B+Tree天然有序的存储特性，LIKE + 右侧模糊匹配虽可以使用到name索引，但模糊匹配后得到的结果变成无序，所以后面条件无法再使用到索引，因此需回表提取出`name like '陆%'`结果集后，只能**回表**，再通过**普通查询**得到`age = 100`的最终结果。

* 引入ICP后：

  在索引内部取到name的结果后，继续判断了age条件，由此减少了一次回表过程，提升了查询效率

  

有了索引下推优化，**可以在有like条件查询的情况下，减少回表次数**。



##### 那什么情况下会发生明明创建了索引，但是执行的时候并没有通过索引呢？

查询优化器

一条SQL语句的查询，可以有不同的执行方案，至于最终选择哪种方案，需要通过优化器进行选择，选择执行成本最低的方案。 在一条单表查询语句真正执行之前，MySQL的查询优化器会找出执行该语句所有可能使用的方案，对比之后找出成本最低的方案。这个成本最低的方案就是所谓的执行计划。 

优化过程大致如下：

 1、根据搜索条件，找出所有可能使用的索引 2、计算全表扫描的代价 3、计算使用不同索引执行查询的代价 4、对比各种执行方案的代价，找出成本最低的那一个



##### DDL、DML、DCL

DDL：数据定义语言，create、drop、alter，操作表、表属性

DML：数据操控语言，insert、delete、update，操作表中的记录

DCL：数据控制语言，grant、revoke，操作数据库用户



##### MySQL锁分类 和 MySQL锁机制

https://blueskykong.blog.csdn.net/article/details/112914594

1. 表锁和行锁

   表锁：对整张表加锁。开始时使用`lock`加锁，之后用到的所有表都会加锁，最后直接通过`unlock tables`释放所有的表锁

   行锁：对数据行加锁。操作过程中，使用到的所有行(索引)都会加锁。如查询二级索引会加锁，回表查聚簇索引时又会加一把锁。

2. 行锁的模式

   读写锁、读写意向锁、自增锁

   **读写锁**

   读锁，共享锁，S锁，加读锁，事务都能读取，但是不能修改，可以多个事务同时对某记录加读锁

   写锁，排他锁，X锁，独占锁，只有拥有该锁的事务能读取和修改，其他事务不行，同一时间只能有一个事务加写锁

   **读写意向锁**

   表锁和行锁虽范围不同，但是会互相冲突。**检测冲突**，如果用遍历的方式，效率很低。因此使用读写意向锁，检测冲突时，只需查看是否有意向锁即可

   意向锁是表级锁，分为读意向锁(IS锁)和写意向锁(IX锁)。事务在加读锁或写锁之前，必须先加上对应的意向锁。

   意向锁之间不会冲突，也不和自增锁冲突。它只会阻塞表级读锁或表级写锁。也不会和行锁冲突。

   **自增锁**(旧版本)

   AI锁，一种表锁，表中有自增列。插入新数据，生成自增列前，先加自增锁，阻塞其他事务的插入。

   自增锁互不兼容

   自增值一旦分配了就会+1，如果事务回滚，自增值也不会回减。所以有时候自增值会出现中断，不连续的情况

   **兼容矩阵**(第一行是已有的锁，第一列是要加的锁)

   <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210413133848734.png" alt="image-20210413133848734" style="zoom:50%;" />

   意向锁之间互不冲突

   共享锁只兼容共享锁和读意向锁兼容，和其他都冲突

   排他锁和所有锁都冲突

   自增锁只和意向锁兼容

3. 行锁的类型(算法)

   **记录锁**(Record Lock)：只锁一条记录

   **间隙锁**(Gap Lock)：加在两个索引之间的锁，或者加在第一个索引之前或最后一个索引之后的间隙。为了防止其他事务在这个范围内插入或修改记录，即保证两次读取这个范围内的数据，不会发生变化，不会出现幻读

   间隙锁之间互不冲突

   **Next-Key锁**：记录锁 + 间隙锁，一般用**左开右闭**来表示Next-Key锁。

   > 如在RR级别下，有如下Next-Key锁`(30, 49](49,50)`，之所以要锁住49和50的间隙，是为了解决幻读问题，如果不锁49和50的间隙，此时插入一条49的记录，就会产生幻读！

   **插入意向锁**：一种特殊的意向锁，表示插入的意向，只有insert时才有。和上面的表级意向锁不是一个概念嗷！

   插入意向锁之间互不冲突。只会和间隙锁、Next-Key锁冲突。

   **冲突矩阵**

   <img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210413133910670.png" alt="image-20210413133910670" style="zoom:50%;" />

   - 插入意向锁不影响其他事务加其他任何锁。也就是说，一个事务已经获取了插入意向锁，对其他事务是没有任何影响的；
   - 插入意向锁与间隙锁和 Next-key 锁冲突。也就是说，一个事务想要获取插入意向锁，如果有其他事务已经加了间隙锁或 Next-key 锁，则会阻塞。

   其他类型的锁的规则较为简单：

   - 间隙锁不和其他锁（不包括插入意向锁）冲突；
   - 记录锁和记录锁冲突，Next-key 锁和 Next-key 锁冲突，记录锁和 Next-key 锁冲突；



##### 数据库悲观锁怎么实现

##### InnoDB行级锁是基于什么样的机制实现的，具体加什么锁，会不会产生死锁，什么样的场景会发生死锁

通过给索引项加锁来实现的。

##### 什么时候用行锁、什么时候用表锁

InnoDB中行锁是基于**索引**实现的，因此MySQL中行级锁不是直接锁记录，而是锁索引的。

在使用聚簇索引或唯一索引时，使用行锁；非索引使用表级锁

##### 数据库的事务？

**ACID**

原子性：事务不可分割，要么都成功，要么全部失败回滚

一致性：事务执行前后保持一致状态

隔离性：事务提交前，对其他事务不可见

持久性：一旦事务提交，则修改永久保存

> 只有满足一致性，事务的结果才是正确的
>
> 无并发，事务串行，隔离性一定满足。此时只要满足原子性，就一定能满足一致性；
>
> 有并发，必须满足原子性和隔离性，才能满足一致性
>
> 持久化是应对数据库崩溃的情况

**原子性依赖WAL**(Write ahead log)，即 redo log，先顺序写日志，后调整磁盘对象

好处：顺序写日志速度快；如果写日志过程中数据库挂了，最多日志出错，不会影响磁盘

**一致性依赖程序猿**，指数据的逻辑合理

**隔离性依赖MVCC或LBCC**，思路就是给数据库做瞬时快照，读用快照，写用锁，从而做到读写并行；LBCC就是读写锁

**持久性依赖磁盘或副本**：持久性是结果

如均分大于80分的学生中，A课程分数大于90的学生数



##### UUID作为主键对索引写的影响

##### 文件很多、用户很多，怎么分表

##### group by，having，where执行顺序

Where, Group By, Having, Order by。

##### 数据库怎么分表？水平分表和垂直分表

水平切分：同一个表的数据拆分到多个结构相同的表中，用来**缓解单个数据库的压力**

> 切分策略：哈希取模、id范围、映射表
>
> 问题：需要使用全局唯一ID、每个分片指定一个ID范围等

垂直切分：一个表按列分成对个表，可以将经常用的列和不经常用的列切分到不同的表中



##### 分库解决了什么，分表解决了什么

<img src="C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210408230437299.png" alt="image-20210408230437299" style="zoom:67%;" />

##### 分表怎么设计主键(一条数据或消息的唯一ID标识)

**原则**

1. 全局唯一性
2. 趋势递增：数据库很多是B+Tree索引，为了提高写入性能
3. 单调递增
4. 信息安全：连续的容易被扒取

123对应不同的场景；3和4是互斥的，无法同时满足

进阶的：有容灾能力、高可用、高性能、延迟低

**方法**

1. UUID：性能高，本地生成，无网络延时；16字节不易存储，信息不安全(基于mac地址生成的UUID可能不安全)，对数据库不友好(太长了，不利于建索引)

2. snowflake雪花算法

* 与指定日期的时间差(毫秒级)，41位
* 集群ID + 机器ID，10位
* 序列，12位

![image-20210408230954895](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210408230954895.png)

优

* 毫秒数在高位，自增在低位，整体递增
* 不依赖第三方系统，以服务方式部署，稳定，性能高
* 可以根据自身业务分配bit位，灵活

缺：强依赖时钟机器，如果机器上时钟回拨，会导致发号重复或服务不可用

3. 数据库生成

   通过设置字段实现ID递增，通过读写MySQL得到ID号

   优

   * 简单，成本小
   * ID递增，灵活

   缺

   * 强依赖DB，DB挂了，服务也没了
   * ID发号性能瓶颈

4. 美团Leaf https://tech.meituan.com/2017/04/21/mt-leaf.html

   1. 对数据库方案的优化

      原方案每次获取ID都得读写一次数据库，造成数据库压力大。

      改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。

      各个业务不同的发号需求用biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。

      主要字段：biz_tag用来区分业务，max_id表示该biz_tag目前所分配的ID号段的最大值，step表示每次分配的号段长度

      ![image-20210409093121141](C:%5CUsers%5Clqy98%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20210409093121141.png)

      优

      * 很方便的线性扩展，性能高
      * ID号是趋势递增的8byte的64位数字，即使DB宕机，短时间内仍可对外服务
      * 可自定义max_id的大小，方便业务的迁移

      缺

      * ID号不够随机，安全性欠佳

      * 仍存在偶尔延时高，tp999偶尔尖刺

        双buffer优化，相当于提前取下一个号段，进缓存

      * DB宕机会造成整个系统不可用

        主从部署

      2. 对雪花算法方案的优化

         > 趋势递增ID不适用订单ID生成，这样可以计算出某公司一天的订单量，不能接收啊

         仍然采用“1 + 41 + 10 + 12”的方式组装ID。结合Zookeeper来使用。

         没看懂。。

##### MVCC

`MVCC (Multiversion Concurrency Control)` 中文全程叫**多版本并发控制**，是现代数据库（包括 `MySQL`、`Oracle`、`PostgreSQL` 等）引擎实现中常用的处理读写冲突的手段，**目的在于提高数据库高并发场景下的吞吐性能**。

如此一来，不同事务并发过程中，`SELECT` 操作可以不加锁而是通过 `MVCC` 机制读取指定的版本历史记录，并通过一些手段保证保证读取的记录值符合事务所处的隔离级别，从而解决并发场景下的读写冲突。

对数据库的任何修改的提交都不会直接覆盖之前的数据，而是产生一个新的版本与老版本共存，使得读取时可以完全不加锁。



**MVCC使得数据库读不会对数据加锁，普通的SELECT请求不会加锁，提高了数据库的并发处理能力**。借助MVCC，数据库可以实现READ COMMITTED，REPEATABLE READ等隔离级别，用户可以查看当前数据的前一个或者前几个历史版本，保证了ACID中的I特性（隔离性)。

只在`Read Committed`和`Repeatable read`两个隔离级别下工作；可以用乐观锁和悲观锁实现；

**事务快照 read view**：用来存储数据库的事务运行情况。查看所有未提交并活跃的事务，最小的XID记在xmin中，最大的记在xman中

**InnoDB的Repeatable Read级别中，会在事务执行第一个select读操作后，创建一个快照**

**InnoDB的Read Committed级别中，事务每条select都会创建一个快照**

**undo-log**：记录数据库变更操作，存储的是老版本的数据。当一个旧的事务需要读取数据时，为了能读取到老版本的数据，需要顺着undo链找到满足其可见性的记录。

在回滚中：undo-log分为insert undo-log和update undo-log

insert undo-log：insert时产生的log，只在事务回滚时需要，事务提交后就可以丢弃

update undo-log：delete和update产生的log，不仅事务回滚需要，一致性读也需要，所以不能随便删除

InnoDB存储引擎在每行后添加3个字段：

**事务ID**：6字节，本次修改的事务的标识符，**顺序严格递增**

**回滚指针**：7字节，写入回滚段的undo-log record，如果一行记录被更新, 则 undo log record 包含 '重建该行记录被更新之前内容' 所必须的信息。

**DB_ROW_ID**：6字节，包含一个随着新行插入而单调递增的行ID, 当由innodb自动产生聚集索引时，聚集索引会包括这个行ID的值，否则这个行ID不会出现在任何索引中。

**可见性比较算法：**

设要读取的行的最后提交事务id(即当前数据行的稳定事务id)为 `trx_id_current`
当前新开事务id为 `new_id`
当前新开事务创建的快照`read view` 中最早的事务id为`up_limit_id`, 最迟的事务id为`low_limit_id`(注意这个low_limit_id=未开启的事务id=当前最大事务id+1)
比较:

- 1.`trx_id_current < up_limit_id`, 这种情况比较好理解, 表示, 新事务在读取该行记录时, 该行记录的稳定事务ID是小于, 系统当前所有活跃的事务, 所以当前行稳定数据对新事务可见, 跳到步骤5.
- 2.`trx_id_current >= trx_id_last`, 这种情况也比较好理解, 表示, 该行记录的稳定事务id是在本次新事务创建之后才开启的, 但是却在本次新事务执行第二个select前就commit了，所以该行记录的当前值不可见, 跳到步骤4。
- 3.`trx_id_current <= trx_id_current <= trx_id_last`, 表示: 该行记录所在事务在本次新事务创建的时候处于活动状态，从up_limit_id到low_limit_id进行遍历，如果trx_id_current等于他们之中的某个事务id的话，那么不可见, 调到步骤4,否则表示可见。
- 4.从该行记录的 DB_ROLL_PTR 指针所指向的回滚段中取出最新的undo-log的版本号, 将它赋值该 `trx_id_current`，然后跳到步骤1重新开始判断。
- 5.将该可见行的值返回。



##### 一、索引优化

索引的数据结构是 B+Tree，而 B+Tree 的查询性能是比较高的，所以建立索引能提升 SQL 的查询性能。

**1、建立普通索引**

对经常出现在 where 关键字后面的表字段建立对应的索引。

**2、建立复合索引**

如果 where 关键字后面常出现的有几个字段，可以建立对应的 复合索引。要注意可以优化的一点是：将单独出现最多的字段放在前面。

例如现在我们有两个字段 a 和 b 经常会同时出现在 where 关键字后面：

```
select * from t where a = ``1` `and b = ``2``;  \* Q1 *\
```

也有很多 SQL 会单独使用字段 a 作为查询条件：

```
select * from t where a = ``2``;  \* Q2 *\
```

此时，我们可以建立复合索引 index(a,b)。因为不但 Q1 可以利用复合索引，Q2 也可以利用复合索引。

**3、最左前缀匹配原则**

如果我们使用的是复合索引，应该尽量遵循 最左前缀匹配原则。MySQL 会一直向右匹配直到遇到范围查询(>、<、between、like)就停止匹配。

假如此时我们有一条 SQL ：

```
select * from t where a = ``1` `and b = ``2` `and c > ``3` `and d = ``4``;
```

那么我们应该建立的复合索引是：index(a,b,d,c) 而不是 index(a,b,c,d)。因为字段 c 是范围查询，当 MySQL 遇到范围查询就停止索引的匹配了。

大家也注意到了，其实 a,b,d 在 SQL 的位置是可以任意调整的，优化器会找到对应的复合索引。

> 还要注意一点的是：最左前缀匹配原则不但是复合索引的最左 N 个字段；也可以是单列（字符串类型）索引的最左 M 个字符。

- 例如我们常说的 like 关键字，尽量不要使用全模糊查询，因为这样用不到索引；
- 所以建议是使用右模糊查询：select * from t where name like '李%'（查询所有姓李的同学的信息）。

**4、索引下推**

很多时候，我们还可以复合索引的 索引下推 来优化 SQL 。

例如此时我们有一个复合索引：index(name,age) ，然后有一条 SQL 如下：

```
select * from user where name like ``'苏%'` `and age = ``10` `and sex = ``'m'``;
```

根据复合索引的最左前缀匹配原则，MySQL 匹配到复合索引 index(name,age) 的 name 时，就停止匹配了；然后接下来的流程就是根据主键回表，判断 age 和 sex 的条件是否同时满足，满足则返回给客户端。

但是由于有索引下推的优化，匹配到 name 时，不会立刻回表；而是先判断复合索引 index(name,age) 中的 age 是否符合条件；符合条件才进行回表接着判断 sex 是否满足，否则会被过滤掉。

那么借着 MySQL 5.6 引入的索引下推优化 ，可以做到减少回表的次数。

**5、覆盖索引**

很多时候，我们还可以 覆盖索引 来优化 SQL 。

**情况一**：SQL 只查询主键作为返回值。

主键索引（聚簇索引）的叶子节点是整行数据，而普通索引（二级索引）的叶子节点是主键的值。

所以当我们的 SQL 只查询主键值，可以直接获取对应叶子节点的内容，而避免回表。

**情况二**：SQL 的查询字段就在索引里。

复合索引：假如此时我们有一个复合索引 index(name,age) ，有一条 SQL 如下：

```
select name,age from t where name like ``'苏%'``;
```

由于是字段 name 是右模糊查询所以可以走复合索引，然后匹配到 name 时，不需要回表，因为 SQL 只是查询字段 name 和 age，所以直接返回索引值就 ok 了。

**6、普通索引**

尽量 使用普通索引 而不是唯一索引。

首先，普通索引和唯一索引的查询性能其实不会相差很多；当然了，前提是要查询的记录都在同一个数据页中，否则普通索引的性能会慢很多。

但是，普通索引的更新操作性能比唯一索引更好；其实很简单，因为普通索引能利用 change buffer 来做更新操作；而唯一索引因为要判断更新的值是否是唯一的，所以每次都需要将磁盘中的数据读取到 buffer pool 中。

**7、前缀索引**

我们要学会巧妙的使用 前缀索引，避免索引值过大。

例如有一个字段是 addr varchar(255)，但是如果一整个建立索引 [ index(addr) ]，会很浪费磁盘空间，所以会选择建立前缀索引 [ index(addr(64)) ]。

建立前缀索引，一定要关注字段的区分度。例如像身份证号码这种字段的区分度很低，只要出生地一样，前面好多个字符都是一样的；这样的话，最不理想时，可能会扫描全表。

前缀索引避免不了回表，即无法使用覆盖索引这个优化点，因为索引值只是字段的前 n 个字符，需要回表才能判断查询值是否和字段值是一致的。

**怎么解决**？

倒序存储：像身份证这种，后面的几位区分度就非常的高了；我们可以这么查询：

select field_list from t where id_card = reverse('input_id_card_string');增加 hash 字段并为 hash 字段添加索引。

**8、干净的索引列**

索引列不能参与计算，要保持索引列“干净”。

假设我们给表 student 的字段 birthday 建立了普通索引。

下面的 SQL 语句不能利用到索引来提升执行效率：

```
select * from student where DATE_FORMAT(birthday,``'%Y-%m-%d'``) = ``'2020-02-02'``;
```

我们应该改成下面这样：

```
select * from student where birthday = STR_TO_DATE(``'2020-02-02'``, ``'%Y-%m-%d'``);
```

**9、扩展索引**

我们应该尽量 扩展索引，而不是新增索引，一个表最好不要超过 5 个索引；一个表的索引越多，会导致更新操作更加耗费性能。

###### 二、SQL 优化

**1、Order By 优化**

order by 后面的字段尽量是带索引的，这样能避免使用 sort_buffer 进行排序。

- 假如有一条 SQL，根据生日查询所有学生的信息：select * from student order by birthday desc;
- 那么为了提升 SQL 的查询性能，我们可以为 birthday 字段建立索引：

```
CREATE INDEX index_birthday ON student(birthday);
```

select 后面不要带上不必要的字段，因为如果单行长度太长导致查询数据太多，MySQL 会利用 rowid 排序来代替全字段排序，这样会导致多了回表的操作。

- 如果我们只是查询学生的姓名、年龄和生日，千万不要写 select *;
- 而是只查询需要的字段：select name, age, birthday from student order by birthday desc;

**2、Join 优化**

- 在使用 join 的时候，应该让小表做驱动表。小表：总数据量最小的表
- 使用 join 语句，最好保证能利用被驱动表的索引，不然只能使用 BNL（Block Nested-Loop Join）算法，还不如不用。
- 启用 BKA（Batched Key Access） 算法，使得 NLJ 算法也能利用上 join_buffer，被驱动表可以批量查询到符合条件的值，然后可以利用 MMR（Multi-Range Read） 的顺序读盘特性来提升回表效率。
- 如果一定要用 join，而且被驱动表没有索引可以使用，那么我们可以利用临时表（create temporary table xx(...)engine=innodb;）来让 BNL 算法转为 BKA 算法，从而提升查询性能。
- join_buffer 是一个无序数组，所以每次判断都需要遍历整个 join_buffer。我们可以在业务端实现 hash join 来提升 SQL 的执行速度。

**3、Group By 优化**

- 如果对 group by 语句的结果没有排序要求，要在语句后面加 order by null。
- 尽量让 group by 过程用上表的索引，不但不需要临时表，还不需要额外的排序。
- 如果 group by 需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大 tmp_table_size 参数，来避免用到磁盘临时表。
- 如果数据量实在太大，使用 SQL_BIG_RESULT 这个提示，来告诉优化器直接使用排序算法得到 group by 的结果。

**4、OR 优化**

在 Innodb 引擎下 or 关键字无法使用组合索引。

假设现在关于订单表有一条 SQL ：

```
select id，product_name from orders where mobile = ``'12345678900'` `or user_id = ``6``;
```

一般我们为了提升上面 SQL 的查询效率，会想着为字段 mobile 和 user_id 建立一个复合索引 index(mobile,user_id)；

可是我们使用 explain 可以发现执行计划里面并没有提示到使用复合索引，所以 or 关键字无法命中 mobile + user_id 的组合索引。

那么我们可以分别为两个字段建立普通索引，然后采用 union 关键字，如下所示：

```
(select id，product_name from orders where mobile = ``'12345678900'``)``union``(select id，product_name from orders where user_id = ``6``);
```

此时 mobile 和 user_id 字段都有索引，查询才最高效。

**5、IN 优化**

in 关键字适合主表大子表小，exist 关键字适合主表小子表大。由于查询优化器的不断升级，很多场景这两者性能差不多一样了，可以尝试改为 join 查询。

假设我们现在有一条 SQL ，要查询 VIP 用户的所有订单数据：

```
select id from orders where user_id in (select id from user where level = ``'VIP'``);
```

我们可以发现不会有任何关于索引的优化，所以我们可以采用 join查询，如下所示：

```
select o.id from orders o join user u on o.user_id = u.id and u.level = ``'VIP'``;
```

此时被驱动表应该是 user，那么可以利用到 user 表的主键索引，即可以使用 BKA 算法来提升 join 查询的性能。

**6、Like 优化**

like 用于模糊查询，但是如果是全模糊查询，将不能命中对应字段的索引。

假设现在关于学生表有一条 SQL：

```
SELECT name,age,birthday FROM student WHERE name like ``'%苏%'``;
```

使用 explain 可以发现执行计划提示查询未命中索引。

因为本来需求就是查询姓张的所有同学信息，所以没必要使用全模糊查询，使用右模糊查询即可。

换成下面的写法：

```
SELECT name,age,birthday FROM student WHERE name like ``'苏%'``;
```

但是产品经理一定要前后模糊匹配呢？全文索引 FULLTEXT 可以尝试一下，但是 MySQL 的全文索引不支持中文查询的。

所以说 Elasticsearch 才是终极武器！

###### 三、数据表设计优化

**1、数据类型：应该选择更简单或者占用空间更小的类型**。

- 整型选择：可以根据长度选择 tinyint、smallint、medium_int，而不是直接使用 int。
- 字符串选择：能确定字符串长度的，尽量使用 char 类型，而不是变长的 varchar 类型。
- 浮点型选择：精度要求比较高的使用 decimal 而不是 double；也可以考虑使用 BIGINT 来保存，小数位保存可以使用乘以整百来解决。
- 日期选择：尽量使用 timestamp 而不是 datetime。

**2、避免空值**：

- NULL 值依然会占用空间，并且会使索引更新更加复杂，更新 NULL 时容易发生索引分裂的现象。
- 可以使用有意义的值来代替 NULL 值，例如 “none” 字符串等等。

**3、超长字符串**：

- 一般超长字符串，varchar 难以存储，我们一般会使用 text 类型。
- 但是 text 类型的字段尽量避免放在主表中，而是抽出来在子表里，用业务主键关联。



##### 内联、左联、右联

内联：inner join

如果想把用户信息、积分、等级都列出来，那么一般会这样写：
`select * from T1 ,T3 where T1.userid = T3.userid`
其实这样的结果等同于`select * from T1 inner join T3 on T1.userid=T3.userid` 
**把两个表中都存在userid的行拼成一行(即内联)**，但后者的效率会比前者高很多，建议用后者(内联)的写法。
`select * from T1 inner join T2 on T1.userid=T2.userid`

左联：left outer join

**显示左表T1中的所有行，并把右表T2中符合条件加到左表T1中;右表T2中不符合条件，就不用加入结果表中，并且NULL表示**。

`select * from T1 left outer join T2 on T1.userid=T2.userid`

右联：right outer join

**显示右表T2中的所有行，并把左表T1中符合条件加到右表T2中;左表T1中不符合条件，就不用加入结果表中，并且NULL表示。**

`select * from T1 right outer join T2 on T1.userid=T2.userid`

全联：full outer join

显示左表T1、右表T2两边中的所有行，即把左联结果表+右联结果表组合在一起，然后过滤掉重复的。

`select * from T1 full outer join T2 on T1.userid=T2.userid`

##### 不等号会用索引吗

where a > 1 这种，如果索引只有字段a，或者a在索引的首位，可能会用索引

不等号有可能使用索引，explain查看是否使用



##### MySQL对数据去重

distinct

```sql
SELECT DISTINCT user_age
FROM user
ORDER BY user_age DESC;
```



group by

```sql
SELECT user_age
FROM user
ORDER BY user_age DESC;
```



##### 查询数据量较大怎么处理



##### redo log、undo log、binlog

redo log，负责**落盘式持久性**，只关心未来，是InnoDB存储引擎层的日志，**记录事务操作的变化，记录修改后的值，可用来恢复数据，保证数据完整性**。更新时，**先写日志，再写磁盘**。redo log日志**大小固定**，写满了就从头开始写，即**写满覆盖**

binlog，**负责副本式持久性**，只关心过去，是MySQL Service层的日志，逻辑日志，以**二进制形式记录语句的原始逻辑**，一份写到一定大小，会换下一份，**不会覆盖**

undo log：**负责原子性**，回滚日志，记录多个历史版本数据，保存事务发生前的数据的一个版本，可用于回滚，MVCC

概述

**binlog**

是啥？记录数据库表结构和表数据的变更，比如增改删创建，但不会记录查select

存啥？binlog存储每条变更的SQL语句、对应的事务id等

作用？复制和恢复数据。保证一主多从数据的一致；数据库挂了用来恢复数据

**redo log**

写啥？修改的请求在内存写完后，落磁盘之前，先写redo log，redo log记载了**这次在哪个页上做了什么修改**

> 写redo log也会有缓存，并且也需要写磁盘，不过是顺序IO，很快

作用？当修改时，写完内存但还没写到磁盘时，如果数据库挂了，可以通过redo log恢复。redo log记录的是物理变化，恢复很快

**binlog 和 redo log区别**

**存储内容**：`binlog`记录的是SQL语句(**逻辑变化**)，而redo log记录的是物理修改的内容(**物理变化**)

**功能**

`redo log`的作用是为**持久化**而生的。写完内存，如果数据库挂了，那我们可以通过`redo log`来恢复内存还没来得及刷到磁盘的数据，将`redo log`加载到内存里边，那内存就能恢复到挂掉之前的数据了。

`binlog`的作用是复制和恢复而生的。

- 主从服务器需要保持数据的一致性，通过`binlog`来同步数据。
- 如果整个数据库的数据都被删除了，`binlog`存储着所有的数据变更情况，那么可以通过`binlog`来对数据进行恢复。

又看到这里，你会想：”如果整个数据库的数据都被删除了，那我可以用`redo log`的记录来恢复吗？“**不能**

因为功能的不同，`redo log` 存储的是物理数据的变更，如果我们内存的数据已经刷到了磁盘了，那`redo log`的数据就无效了。所以`redo log`不会存储着**历史**所有数据的变更，**文件的内容会被覆盖的**。

**写入的细节**







##### InnoDB怎么保证崩溃恢复能力？

两阶段日志提交

##### 自增ID和UUID区别

自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。

插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。
自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。
**而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。**

- 除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？

由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。
**显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。**
链接：https://www.zhihu.com/question/397289720/answer/1246287314

##### 自增ID申请完了会发生什么

自增ID达到上限用完了之后，分为两种情况：

1. 如果设置了主键，那么将会报错主键冲突。

2. 如果没有设置主键，数据库则会帮我们自动生成一个全局的row_id，新数据会**覆盖**老数据

主键尽量设置为bigint

##### explain里面有哪些字段

explain：模拟优化器执行SQL语句，在select前加上explain可以返回执行的一些信息，但不是正真执行这个SQL

字段(大概看一下)

id：id列的编号是select的序列号，有几个select就有几个id，并且id的顺序是按select出现的顺序增长的。id越大执行优先级越高，id相同则从上往下执行，id为NULL最后执行。

select type：对应行是简单还是复杂的查询

table：表示正在访问哪个表

type：这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行，查找数据行对应的大概范围。

possible_keys：可能会使用哪些查询来查找

keys：实际采用哪个索引对该表访问

rows：估计要读取并检测的行数，不是结果集的行数

Extra：额外信息



##### MySQL主从集群？







##### SQL语句很慢，为什么？

偶尔慢：刷新脏页(redo log写满了、内存不够了申请内存)、拿不到锁

一直很慢：没用到索引->最左前缀？索引不能表达式、函数、数据库选错了索引



#### MyBatis(暂时不看)

##### 如何实现动态SQL

##### 框架的好处，怎么连数据库

##### MyBatis的缓存

##### MyBatis的`#`和`$`的区别

##### MyBatis的接口是怎么和XML文件联系上的

##### 为什么用MyBatis，与JDBC、Hibernate区别说



##### 测试

##### CPU 100%如何排查

##### 如何设计压测？



##### 项目相关

##### 整个项目的流程

属性校验怎么做的？javax.validation

```
    <dependency>
      <groupId>org.hibernate</groupId>
      <artifactId>hibernate-validator</artifactId>
      <version>5.2.4.Final</version>
    </dependency>
```

定义一个validateResult类，里面包括是否有错、存放错误的map；在validatorImpl中，通过遍历将有错的对象放到set中，遍历set得到错误的信息和对象的名字，放到result的map中，返回。

使用的话直接在属性上加注解：@NotBlank、@NotNull、@Min、@Max



通用错误如何定义？接口，提供getcode,getmsg,setmsg；通过枚举类，定义错误类型，错误码，默认的错误信息，可以通过set修改具体错误信息；错误码根据具体业务定义不同的开头，类似http状态码



通用返回类型？包含status(success和fail)和Object类型的data，用于返回统一的类型



**用户模块的设计？**

数据库表，用户信息表和密码表

service层提供：注册(入库重复主键手机号验证)、用户登录验证(手机号和验证码校验、校验密码是否匹配)、getByID、getByCache(从redis查，若没有就set进去，设置10分钟失效时间)

controller层

* 用户注册接口
  * @RequestMapping+@RequestBody+@RequestParam
  * 首先，从session中，验证手机号和对应的验证码是否符合，设置相应属性值，密码用md5加密，最后返回

* 获取验证码otp接口：通过random生成 0 ~ 899999随机数，再加上100000，即为随机6位验证码；将手机号和验证码存到session中，发短信

* 用户登录接口：入参校验是否为空，调用服务层的登录验证，校验手机和验证码；生成token，这里用UUID，全局唯一；将token和用户信息存入redis，设置1h过期时间



**商品模块的设计？**

service层：创建、商品列表浏览、库存扣减、销量增加、查缓存、异步更新缓存、库存回补、初始化库存流水

库存扣减：从redis中扣，





##### 秒杀接口如何隐藏

##### 怎么保证高可用？怎么负载均衡？

##### 项目中Redis怎么用的？缓存怎么刷新的？

##### 怎么保证不会超卖？

##### 同一个订单有多个秒杀商品，其中一个库存没了怎么办

##### 怎么保证操作原子性



查询量上百万，优化MySQL？Redis缓存？

1. 本地缓存：适合结果集很小的公共数据，改造成本低，查询快
2. redis缓存：适合缓存规模较大的数据，
3. MySQL索引：极大加快查询速度
4. MySQL主从：查询结果多变，无法缓存；若用redis，命中率很低；要考虑主从同步延迟







#### 其他

##### 分布式并发控制，分布式锁分布式并发控制，分布式锁

##### 数据量很大，如果存在一张表，会出现很多新属性，怎么设计数据库表

##### 40亿个qq号怎么查重？布隆过滤器多大？

##### for循环连续100次更新一行记录，会出现阻塞的情况？怎么编码解决？

手动继承AQS

##### 一个分布式项目，需要哪些模块

##### 一个项目的两个.jar包引用了c的不同版本1.0.0和2.0.0，这时候依赖的c版本是什么